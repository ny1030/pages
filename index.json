[{"content":"ユースケース別 Operation（運用） RDS_特定の認証局証明書を利用しているインスタンスの列挙 WIP RDS_利用可能な認証局証明書の確認（RDS for PostgreSQL） 結果が大量にあるため、下記例では --engine で RDS for PostgreSQL のみに絞っている aws rds describe-db-engine-versions --engine postgres | jq -r \u0026#39;.DBEngineVersions[] | [.Engine, .EngineVersion, .SupportedCACertificateIdentifiers] | @csv \u0026#39; ","permalink":"https://ny1030.github.io/pages/posts/engineering/aws-cli-%E3%82%B9%E3%83%8B%E3%83%9A%E3%83%83%E3%83%88%E9%9B%86/","summary":"ユースケース別 Operation（運用） RDS_特定の認証局証明書を利用しているインスタンスの列挙 WIP RDS_利用可能な認証局証明書の確認（RDS for PostgreSQL） 結果が大量にあるため、下記例では --engine で RDS for PostgreSQL のみに絞っている aws rds describe-db-engine-versions --engine postgres | jq -r \u0026#39;.DBEngineVersions[] | [.Engine, .EngineVersion, .SupportedCACertificateIdentifiers] | @csv \u0026#39; ","title":"AWS CLI スニペット集"},{"content":"概要 AWS RDS/Auroraを利用しており、クライアントがSSL/TLSを使って接続している場合、Amazon RDS 認証局証明書（rds-ca-2019）の更新が2024/08までに必要。 (AWS Document) SSL/TLS 証明書のローテーション\n対象の調査 対象となるRDSの列挙 RDS_特定の認証局証明書を利用しているインスタンスの列挙 のコマンドで対象インスタンスを調べる 上記結果で、rds-ca-2019を利用しているものが更新が必要。また、更新可能な証明書は [RDS_利用可能な認証局証明書の確認（RDS for PostgreSQL）](../AWS%20CLI%20スニペット集/index.md#RDS_利用可能な認証局証明書の確認（RDS for PostgreSQL）) のように確認できる\nSSL/TLSを利用しているか調査 (AWS Document) PostgreSQL DB インスタンスで SSL を使用する まずはRDS側でSSL接続を利用しているかどうかは rds.force_ssl で確認可能\n$ aws rds describe-db-cluster-parameters --db-cluster-parameter-group-name default.aurora-postgresql13 | jq \u0026#39;.Parameters[] | select(.ParameterName == \u0026#34;rds.force_ssl\u0026#34;)\u0026#39; 出力例： ParameterValueが0のため、SSLは必須ではない。SSL接続が確立できない場合はSSL接続が利用されない。\n{ \u0026#34;ParameterName\u0026#34;: \u0026#34;rds.force_ssl\u0026#34;, \u0026#34;ParameterValue\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Force SSL connections.\u0026#34;, \u0026#34;Source\u0026#34;: \u0026#34;system\u0026#34;, ... } ","permalink":"https://ny1030.github.io/pages/posts/engineering/aws%E9%81%8B%E7%94%A8_rdsaurora-ssltls-certificate-expires%E3%81%AE%E5%AF%BE%E5%BF%9C/","summary":"概要 AWS RDS/Auroraを利用しており、クライアントがSSL/TLSを使って接続している場合、Amazon RDS 認証局証明書（rds-ca-2019）の更新が2024/08までに必要。 (AWS Document) SSL/TLS 証明書のローテーション\n対象の調査 対象となるRDSの列挙 RDS_特定の認証局証明書を利用しているインスタンスの列挙 のコマンドで対象インスタンスを調べる 上記結果で、rds-ca-2019を利用しているものが更新が必要。また、更新可能な証明書は [RDS_利用可能な認証局証明書の確認（RDS for PostgreSQL）](../AWS%20CLI%20スニペット集/index.md#RDS_利用可能な認証局証明書の確認（RDS for PostgreSQL）) のように確認できる\nSSL/TLSを利用しているか調査 (AWS Document) PostgreSQL DB インスタンスで SSL を使用する まずはRDS側でSSL接続を利用しているかどうかは rds.force_ssl で確認可能\n$ aws rds describe-db-cluster-parameters --db-cluster-parameter-group-name default.aurora-postgresql13 | jq \u0026#39;.Parameters[] | select(.ParameterName == \u0026#34;rds.force_ssl\u0026#34;)\u0026#39; 出力例： ParameterValueが0のため、SSLは必須ではない。SSL接続が確立できない場合はSSL接続が利用されない。\n{ \u0026#34;ParameterName\u0026#34;: \u0026#34;rds.force_ssl\u0026#34;, \u0026#34;ParameterValue\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;Description\u0026#34;: \u0026#34;Force SSL connections.\u0026#34;, \u0026#34;Source\u0026#34;: \u0026#34;system\u0026#34;, ... } ","title":"AWS運用_RDS,Aurora SSL,TLS certificate expiresの対応"},{"content":"Background 本番環境だけ作っていたシステムがあり、DBMSなどの大幅なバージョンアップを計画しており、流石にテストが必要ということで、検証環境を別AWSアカウントに作ることになった。 EC2, RDSなどのリソースしかない簡単なスタックだが、SGの設定なども移行するとなると割と面倒なので、既存のリソースを使った方法で楽に作れないかを模索。\n以下のFormer2という既存のリソースからCloudFormationやTerraformのコードを生成するツールがあることを発見。 https://github.com/iann0036/former2\nFormer2の使用 以下のWebサイトで自身のAWSアカウントのクレデンシャルなどを登録すると、SDK APIでリソースを解析してテンプレートを生成してくれる模様 https://former2.com/#\nただ、流石に上記のサイトにクレデンシャルを登録するのは気が引けるので、Dockerを利用してスタンドアロンでFormer2を使う方法を試した。\nFormer2（スタンドアロン版）の使い方 以下リポジトリをクローンする https://github.com/iann0036/former2\nrootディレクトリで docker compose up -d でビルドおよび起動をする\n127.0.0.1 をブラウザで開くと、Former2が表示される\n案内の通り、Chrome拡張機能をインストール https://chrome.google.com/webstore/detail/former2-helper/fhejmeojlbhfhjndnkkleooeejklmigi\n解析対象のリソースがあるAWSのクレデンシャルを入力\nCloud Formation のパラメータを入力、とりあえずはここは未設定 最後にここの画面の緑色の Scan ボタンを押すとスキャンが始まる、不足している権限があると都度ポップアップで表示される。 また右上にリージョンの設定があり、デフォルトではバージニア北部になっているので要確認。 スキャンの途中でサイドバーからRDSを見てみると すげえ！！ちゃんと表示されてる。 Describeの内容を表示してるだけなんだけど、自分の手元でこうやって一覧を見れるのはテンションが上がる。\n気になったところ スキャンする際に全てのリソースがスキャンされるのか？ 全てをスキャンするのは便利であるものの、自身のM1 MacBookProだと、10分経過したあたりでブラウザが一度ハングした。全てスキャンした後の画面ではフィルタの機能などはあるのはわかるが、スキャンする前に特定のリソースをスキャンできないのか？\n調べた＆使ってみた結果、出来なさそう。なのでリソースが沢山ある場合はスキャンに数時間かかることと、キャッシュされてない状態で開いた際にリソース情報のロードに時間を要することは念頭に置いて使った方がいい。\nスキャンした情報は永続化できる？ Dockerプロセスを停止=\u0026gt;起動して試したが、スキャンした情報は残ってたので永続化はされてそう。ただし、Former2起動直後はリソース情報をロード中で、何も出てこないように見えるので注意。\nテンプレート作成可能なリソースタイプ EC2やRDSなど基本的なサービスは対応しているが、以下のダッシュボードから確認した限りだとSecurity Groupなどは作成できない。 SecurityGroupに色んなルールを追加している場合は移行がめんどかったりするので、痒い所に手が届かない感じはする。\n自分はChatGPTの力を借りて、SecurityGroupのルールを別アカウントに移行してくれるPythonスクリプトを作成した。\nimport boto3 from dotenv import load_dotenv import botocore import os # .envファイルから環境変数を読み込む load_dotenv() source_account_access_key = os.getenv(\u0026#34;SOURCE_ACCESS_KEY\u0026#34;) source_account_secret_key = os.getenv(\u0026#34;SOURCE_SECRET_KEY\u0026#34;) dest_account_access_key = os.getenv(\u0026#34;DEST_ACCESS_KEY\u0026#34;) dest_account_secret_key = os.getenv(\u0026#34;DEST_SECRET_KEY\u0026#34;) # INPUT情報 source_security_group_id = \u0026#39;sg-xxxxx\u0026#39; source_region = \u0026#39;ap-northeast-1\u0026#39; dest_security_group_name = \u0026#39;security-group-sample\u0026#39; dest_security_group_description = \u0026#39;Created by SDK\u0026#39; dest_region = \u0026#39;ap-northeast-1\u0026#39; dest_vpc_id = \u0026#39;vpc-xxxxxxx\u0026#39; # 元のアカウントからSecurity Group情報を取得 source_ec2 = boto3.client(\u0026#39;ec2\u0026#39;, aws_access_key_id=source_account_access_key, aws_secret_access_key=source_account_secret_key, region_name=source_region) response = source_ec2.describe_security_groups(GroupIds=[source_security_group_id]) source_security_group_info = response[\u0026#39;SecurityGroups\u0026#39;][0] # 目的のアカウントでSecurity Groupを作成 dest_ec2 = boto3.client(\u0026#39;ec2\u0026#39;, aws_access_key_id=dest_account_access_key, aws_secret_access_key=dest_account_secret_key, region_name=dest_region) response = dest_ec2.create_security_group( GroupName=dest_security_group_name, Description=dest_security_group_description, VpcId=dest_vpc_id ) for rule in source_security_group_info[\u0026#39;IpPermissions\u0026#39;]: modified_rule = rule.copy() # ルールをコピーして編集 if \u0026#39;UserIdGroupPairs\u0026#39; in modified_rule: # 移行先に存在しないセキュリティグループを除外する modified_rule[\u0026#39;UserIdGroupPairs\u0026#39;] = [ pair for pair in modified_rule[\u0026#39;UserIdGroupPairs\u0026#39;] if pair.get(\u0026#39;GroupId\u0026#39;) == response[\u0026#39;GroupId\u0026#39;] # 移行先セキュリティグループに対するルールのみ残す ] try: if modified_rule.get(\u0026#39;IpRanges\u0026#39;) or modified_rule.get(\u0026#39;Ipv6Ranges\u0026#39;) or modified_rule.get(\u0026#39;UserIdGroupPairs\u0026#39;): # ルールに許可するエントリが存在する場合のみ処理を実行 dest_ec2.authorize_security_group_ingress( GroupId=response[\u0026#39;GroupId\u0026#39;], IpPermissions=[modified_rule] ) except botocore.exceptions.ClientError as e: error_code = e.response[\u0026#39;Error\u0026#39;][\u0026#39;Code\u0026#39;] if error_code == \u0026#39;InvalidPermission.Duplicate\u0026#39;: print(\u0026#34;Rule already exists in the destination security group. Skipping.\u0026#34;) elif error_code == \u0026#39;DependencyViolation\u0026#39;: print(\u0026#34;Dependency violation. Skipping rule with source SG:\u0026#34;, modified_rule.get(\u0026#39;UserIdGroupPairs\u0026#39;)) else: print(\u0026#34;Error:\u0026#34;, e.response[\u0026#39;Error\u0026#39;][\u0026#39;Message\u0026#39;]) print(\u0026#34;Security Group migration completed.\u0026#34;) ","permalink":"https://ny1030.github.io/pages/posts/engineering/aws%E3%83%AA%E3%82%BD%E3%83%BC%E3%82%B9%E3%81%AE%E7%A7%BB%E8%A1%8C%E3%82%92former2%E3%81%A7%E5%AE%9F%E6%96%BD/","summary":"Background 本番環境だけ作っていたシステムがあり、DBMSなどの大幅なバージョンアップを計画しており、流石にテストが必要ということで、検証環境を別AWSアカウントに作ることになった。 EC2, RDSなどのリソースしかない簡単なスタックだが、SGの設定なども移行するとなると割と面倒なので、既存のリソースを使った方法で楽に作れないかを模索。\n以下のFormer2という既存のリソースからCloudFormationやTerraformのコードを生成するツールがあることを発見。 https://github.com/iann0036/former2\nFormer2の使用 以下のWebサイトで自身のAWSアカウントのクレデンシャルなどを登録すると、SDK APIでリソースを解析してテンプレートを生成してくれる模様 https://former2.com/#\nただ、流石に上記のサイトにクレデンシャルを登録するのは気が引けるので、Dockerを利用してスタンドアロンでFormer2を使う方法を試した。\nFormer2（スタンドアロン版）の使い方 以下リポジトリをクローンする https://github.com/iann0036/former2\nrootディレクトリで docker compose up -d でビルドおよび起動をする\n127.0.0.1 をブラウザで開くと、Former2が表示される\n案内の通り、Chrome拡張機能をインストール https://chrome.google.com/webstore/detail/former2-helper/fhejmeojlbhfhjndnkkleooeejklmigi\n解析対象のリソースがあるAWSのクレデンシャルを入力\nCloud Formation のパラメータを入力、とりあえずはここは未設定 最後にここの画面の緑色の Scan ボタンを押すとスキャンが始まる、不足している権限があると都度ポップアップで表示される。 また右上にリージョンの設定があり、デフォルトではバージニア北部になっているので要確認。 スキャンの途中でサイドバーからRDSを見てみると すげえ！！ちゃんと表示されてる。 Describeの内容を表示してるだけなんだけど、自分の手元でこうやって一覧を見れるのはテンションが上がる。\n気になったところ スキャンする際に全てのリソースがスキャンされるのか？ 全てをスキャンするのは便利であるものの、自身のM1 MacBookProだと、10分経過したあたりでブラウザが一度ハングした。全てスキャンした後の画面ではフィルタの機能などはあるのはわかるが、スキャンする前に特定のリソースをスキャンできないのか？\n調べた＆使ってみた結果、出来なさそう。なのでリソースが沢山ある場合はスキャンに数時間かかることと、キャッシュされてない状態で開いた際にリソース情報のロードに時間を要することは念頭に置いて使った方がいい。\nスキャンした情報は永続化できる？ Dockerプロセスを停止=\u0026gt;起動して試したが、スキャンした情報は残ってたので永続化はされてそう。ただし、Former2起動直後はリソース情報をロード中で、何も出てこないように見えるので注意。\nテンプレート作成可能なリソースタイプ EC2やRDSなど基本的なサービスは対応しているが、以下のダッシュボードから確認した限りだとSecurity Groupなどは作成できない。 SecurityGroupに色んなルールを追加している場合は移行がめんどかったりするので、痒い所に手が届かない感じはする。\n自分はChatGPTの力を借りて、SecurityGroupのルールを別アカウントに移行してくれるPythonスクリプトを作成した。\nimport boto3 from dotenv import load_dotenv import botocore import os # .envファイルから環境変数を読み込む load_dotenv() source_account_access_key = os.getenv(\u0026#34;SOURCE_ACCESS_KEY\u0026#34;) source_account_secret_key = os.getenv(\u0026#34;SOURCE_SECRET_KEY\u0026#34;) dest_account_access_key = os.getenv(\u0026#34;DEST_ACCESS_KEY\u0026#34;) dest_account_secret_key = os.","title":"AWSリソースの移行をFormer2で実施"},{"content":"諸々の理由で業務システムのデータベース製品にSQL Serverを使っており、そこで問題が起きた際に解析できるようのメモ。\nRDS for SQL Server を使ってる場合 AWSのRDSでSQL Serverを使っている場合は、AWSネイティブの以下モニタリング機能を使うのがベター。\n拡張モニタリング CPU使用率の内訳が見れる（idleやsys, usrなど） Performance Insight 時系列でロングランしているSQLや何で待機しているか等の情報が見れる どちらもゼロダウンタイムで有効化ができる1。 GCPやAzureは調べてないが、Azureは少なくとも同様のモニタリング機能があるのではと想定。\n調査用のSQL クラウド・オンプレ関わらず、SQL Serverにログインできるのであれば、以下クエリで各種調査を行う。\n所要時間が大きいSQL、CPU時間など SQLの性能問題を解析する際は所要時間が大きいものから一般的には解析していくため、以下のSQLでトータルの所要時間を表す「TotalElapsedTime(sec)」をまず確認。 あわせて、CPU時間やIO回数などの情報も取得している。とりあえず所要時間だけ見たい際はコメントアウトするのが良い。\nSELECT TOP 100 t1.total_worker_time / t1.execution_count / 1000 as \u0026#34;avg cputime(ms)\u0026#34;, t1.max_worker_time /1000 as \u0026#34;max cputime(ms)\u0026#34;, t1.total_worker_time / 1000 as \u0026#34;total cputime(ms)\u0026#34;, t1.total_elapsed_time / 1000 as \u0026#34;TotalElapsedTime(sec)\u0026#34;, t1.total_logical_reads / t1.execution_count as \u0026#34;avg read count\u0026#34;, t1.max_logical_reads as \u0026#34;max read count\u0026#34;, t1.total_logical_reads as \u0026#34;total read count\u0026#34;, t1.execution_count as \u0026#34;exec count\u0026#34;, t2.text as \u0026#34;sql text\u0026#34;, t3.query_plan as \u0026#34;query plan\u0026#34; FROM sys.dm_exec_query_stats as t1 cross apply sys.dm_exec_sql_text(t1.sql_handle) as t2 outer apply sys.dm_exec_query_plan(t1.plan_handle) as t3 WHERE t2.text NOT LIKE \u0026#39;%dm_exec_query_stats%\u0026#39; ORDER BY t1.total_worker_time DESC ; ページサイズ・断片化情報 テーブルやIndexのオブジェクトが肥大化・断片化しているか確認する\nSELECT S.name AS \u0026#39;schema name\u0026#39;, O.name AS \u0026#39;table name\u0026#39;, IDX.name AS \u0026#39;index name\u0026#39;, IDXPS.avg_fragmentation_in_percent AS \u0026#39;fragmentation(%)\u0026#39;, IDXPS.page_count AS \u0026#39;page count\u0026#39; FROM sys.dm_db_index_physical_stats (DB_ID(),null,null,null,null) AS IDXPS LEFT OUTER JOIN sys.objects AS O ON IDXPS.object_id = O.object_id LEFT OUTER JOIN sys.schemas AS S ON O.schema_id = S.schema_id LEFT OUTER JOIN sys.indexes AS IDX ON IDXPS.object_id = IDX.object_id AND IDXPS.index_id = IDX.index_id WHERE O.type = \u0026#39;U\u0026#39; AND IDX.index_id \u0026gt; 0 ORDER BY IDXPS.avg_fragmentation_in_percent DESC ; DB インスタンスの設定\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://ny1030.github.io/pages/posts/engineering/microsoft-sql-server%E3%81%AE%E3%83%91%E3%83%95%E3%82%A9%E3%83%BC%E3%83%9E%E3%83%B3%E3%82%B9%E8%A7%A3%E6%9E%90/","summary":"諸々の理由で業務システムのデータベース製品にSQL Serverを使っており、そこで問題が起きた際に解析できるようのメモ。\nRDS for SQL Server を使ってる場合 AWSのRDSでSQL Serverを使っている場合は、AWSネイティブの以下モニタリング機能を使うのがベター。\n拡張モニタリング CPU使用率の内訳が見れる（idleやsys, usrなど） Performance Insight 時系列でロングランしているSQLや何で待機しているか等の情報が見れる どちらもゼロダウンタイムで有効化ができる1。 GCPやAzureは調べてないが、Azureは少なくとも同様のモニタリング機能があるのではと想定。\n調査用のSQL クラウド・オンプレ関わらず、SQL Serverにログインできるのであれば、以下クエリで各種調査を行う。\n所要時間が大きいSQL、CPU時間など SQLの性能問題を解析する際は所要時間が大きいものから一般的には解析していくため、以下のSQLでトータルの所要時間を表す「TotalElapsedTime(sec)」をまず確認。 あわせて、CPU時間やIO回数などの情報も取得している。とりあえず所要時間だけ見たい際はコメントアウトするのが良い。\nSELECT TOP 100 t1.total_worker_time / t1.execution_count / 1000 as \u0026#34;avg cputime(ms)\u0026#34;, t1.max_worker_time /1000 as \u0026#34;max cputime(ms)\u0026#34;, t1.total_worker_time / 1000 as \u0026#34;total cputime(ms)\u0026#34;, t1.total_elapsed_time / 1000 as \u0026#34;TotalElapsedTime(sec)\u0026#34;, t1.total_logical_reads / t1.execution_count as \u0026#34;avg read count\u0026#34;, t1.max_logical_reads as \u0026#34;max read count\u0026#34;, t1.total_logical_reads as \u0026#34;total read count\u0026#34;, t1.execution_count as \u0026#34;exec count\u0026#34;, t2.","title":"Microsoft SQL Serverのパフォーマンス解析"},{"content":"M1 MacBookProでOracle社のSQL Developer1というSQLクライアントを使っていると、上手く出来なかったり、頻繁に異常終了することがある。\n対処法として、 arch コマンドを使って起動することで、安定して利用できることを確認。\narch -x86_64 /Applications/SQLDeveloper.app/Contents/MacOS/sqldeveloper.sh SQL Developer\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://ny1030.github.io/pages/posts/engineering/m1-mac%E3%81%A7oracle-sql-developer%E3%82%92%E4%BD%BF%E3%81%86%E5%A0%B4%E5%90%88/","summary":"M1 MacBookProでOracle社のSQL Developer1というSQLクライアントを使っていると、上手く出来なかったり、頻繁に異常終了することがある。\n対処法として、 arch コマンドを使って起動することで、安定して利用できることを確認。\narch -x86_64 /Applications/SQLDeveloper.app/Contents/MacOS/sqldeveloper.sh SQL Developer\u0026#160;\u0026#x21a9;\u0026#xfe0e;","title":"M1 MacでOracle SQL Developerを使う場合"},{"content":"General Use {USER_NAME } というユーザのオブジェクト権限確認 SELECT grantee, table_name, privilege FROM dba_tab_privs WHERE grantee = \u0026#39;USER_NAME\u0026#39;; SYSDBA権限をもつユーザ確認 SELECT * FROM V$PWFILE_USERS; 初期化パラメータの一覧 SELECT name,display_value,default_value,isdefault,description FROM V$PARAMETER; 隠しパラメータを調べる select ksppinm as \u0026#34;parameter\u0026#34;, ksppstvl as \u0026#34;value\u0026#34; from x$ksppi join x$ksppcv using (indx) where ksppinm = \u0026#39;{隠しパラメータ名}\u0026#39;; アクセスできるテーブル一覧 SELECT * FROM ALL_TABLES ORDER BY OWNER,TABLE_NAME; Specific Use Parameter 気になるパラメータチェック SELECT name,display_value,default_value,isdefault,description FROM V$PARAMETER WHERE name IN (\u0026#39;client_statistics_level\u0026#39;) OR name like \u0026#39;_optim%\u0026#39;; 気になる隠しパラメータ select ksppinm as \u0026#34;parameter\u0026#34;, ksppstvl as \u0026#34;value\u0026#34; from x$ksppi join x$ksppcv using (indx) where ksppinm IN (\u0026#39;_optimizer_use_stats_on_conventional_dml\u0026#39;,\u0026#39;_optimizer_gather_stats_on_conventional_dml\u0026#39;); SQL Tuning 特定のSQL_IDのActualの実行計画を確認する SHOW parameter statistics_level;でstatistics_levelがtypicalの場合： - ALTER SESSION SET statistics_level=all; - \u0026lt;\u0026lt;対象のSQLを実行\u0026gt;\u0026gt; SELECT * FROM table(DBMS_XPLAN.DISPLAY_CURSOR(`上記で実行したSQLのSQL_ID`, format=\u0026gt;``\u0026#39;ALL ALLSTATS LAST\u0026#39;``)); 補足：\nstatistics_level　をALLに変えてから SQLを再実行するのが重要\nSQLを再実行する際に、全く同じテキストだとSQL_IDが変わらず statistics_level = typical の情報しか得られないのでコメントを入れるなどして別のSQLとして認識させることでActualの情報が取れる\n最後のSQLを実行した際に、A-Rows, A-Timeのカラムが表示されていればActualの情報が取れている。\n実行したSQLのSQL_IDを調べる SQL文にコメントを入れて v$sql から探す\nSELECT * FROM AREA WHERE STORE = \u0026#39;117666\u0026#39; /* FINDME */; -- Sample SQL to find SELECT SQL_ID,FIRST_LOAD_TIME,SQL_TEXT FROM v$sql WHERE SQL_TEXT LIKE \u0026#39;%FINDME%\u0026#39;; 実行された時刻（FIRST_LOAD_TIME）を使って `v$sql` から探す SELECT SQL_ID,FIRST_LOAD_TIME,SQL_TEXT FROM v$sql ORDER BY FIRST_LOAD_TIME DESC; ","permalink":"https://ny1030.github.io/pages/posts/engineering/oracle-sqls/","summary":"General Use {USER_NAME } というユーザのオブジェクト権限確認 SELECT grantee, table_name, privilege FROM dba_tab_privs WHERE grantee = \u0026#39;USER_NAME\u0026#39;; SYSDBA権限をもつユーザ確認 SELECT * FROM V$PWFILE_USERS; 初期化パラメータの一覧 SELECT name,display_value,default_value,isdefault,description FROM V$PARAMETER; 隠しパラメータを調べる select ksppinm as \u0026#34;parameter\u0026#34;, ksppstvl as \u0026#34;value\u0026#34; from x$ksppi join x$ksppcv using (indx) where ksppinm = \u0026#39;{隠しパラメータ名}\u0026#39;; アクセスできるテーブル一覧 SELECT * FROM ALL_TABLES ORDER BY OWNER,TABLE_NAME; Specific Use Parameter 気になるパラメータチェック SELECT name,display_value,default_value,isdefault,description FROM V$PARAMETER WHERE name IN (\u0026#39;client_statistics_level\u0026#39;) OR name like \u0026#39;_optim%\u0026#39;; 気になる隠しパラメータ select ksppinm as \u0026#34;parameter\u0026#34;, ksppstvl as \u0026#34;value\u0026#34; from x$ksppi join x$ksppcv using (indx) where ksppinm IN (\u0026#39;_optimizer_use_stats_on_conventional_dml\u0026#39;,\u0026#39;_optimizer_gather_stats_on_conventional_dml\u0026#39;); SQL Tuning 特定のSQL_IDのActualの実行計画を確認する SHOW parameter statistics_level;でstatistics_levelがtypicalの場合： - ALTER SESSION SET statistics_level=all; - \u0026lt;\u0026lt;対象のSQLを実行\u0026gt;\u0026gt; SELECT * FROM table(DBMS_XPLAN.","title":"Oracle SQLs"},{"content":"URL: https://www.googletagmanager.com/gtag/js?id={id} Referrer Policy: strict-origin-when-cross-origin\nsudo mv /opt/bitnami/drupal/sites/default/default.services.yml /opt/bitnami/drupal/sites/default/services.yml ","permalink":"https://ny1030.github.io/pages/posts/engineering/ga%E5%85%A5%E3%82%8C%E3%82%8B%E6%99%82%E3%81%AEcors%E5%AF%BE%E5%BF%9C/","summary":"URL: https://www.googletagmanager.com/gtag/js?id={id} Referrer Policy: strict-origin-when-cross-origin\nsudo mv /opt/bitnami/drupal/sites/default/default.services.yml /opt/bitnami/drupal/sites/default/services.yml ","title":"GA入れる時のCORS対応"},{"content":"※順次追記\nContext jsランタイム環境であるDenoが最近目にするので、勉強のためにNodejsで書いたコードをDenoに引越ししてみる。\nDeno: https://deno.land/\n処理別の実装方法 JSONのconfigをスクリプトに読み込む Nodejs node-config モジュールを使って以下のように、JSONファイルをオブジェクトとして読み込む。 const conf = require(\u0026#39;config\u0026#39;); //JSONファイルは ./config/default.json const INTERVAL = conf.interval; const LOWER_LIMIT = conf.lowerLimit; const UPPER_LIMIT = conf.upperLimit; Deno 最も簡単と思われる方法は以下のようにnpmをインポートする。 import config from \u0026#34;npm:config\u0026#34;; console.log(config); ただし、この機能自体はDenoが作られた経緯（脱npm）とは反するもので、Deno社も渋々npmをサポートしているように見える。1\nまた、npmのサポートは実験的な機能でまだサポートされてないパッケージもあるので、極力使わないほうがいいかもしれない。\n方法1. Denoで公開されているModuleを使う npmが使えない -\u0026gt; ならDenoで公開しているModuleを使う、という方法。Moduleは以下で公開されている。\nhttps://deno.land/x\nconfigで検索して出てきたModuleを使ってみる。Documentに書いてる例を参考に、以下のような構造のディレクトリを作成する。\n{project_root} ├── app1.ts └── config └── prod └── app1.json 設定ファイルが app1.json で、スクリプトが app1.ts とする。\nimport { Config, CONFIG_FORMATTERS, ConfigFormatter, loadConfig, LoadConfigOptions, } from \u0026#34;https://deno.land/x/load_config_files@0.3.0/mod.ts\u0026#34;; const options: LoadConfigOptions = { verbose: false }; const [formatterId, configRootPath, ...segments] = Deno.args; const configRootUrl = new URL(configRootPath, \u0026#34;file:\u0026#34; + Deno.cwd()); const config: Config = await loadConfig(configRootUrl, segments, options); const formatter: ConfigFormatter = CONFIG_FORMATTERS[formatterId]; const output: string = await formatter(config); console.log(output); 割りと長いが、実行時に以下のようなオプションを引数で渡すことで読み込みできることを確認。\nformatterId: 設定ファイルをパースする時の形式 (shell, json, spring_shell) configRootPath: configファイルのRootフォルダのパス（例の場合はconfig） segments: config配下にフォルダ/ファイルが複数ある時の絞り込むオプションかと思う。例では、prodとapp1を指定。 実行時のコマンドは以下のようになる。\ndeno run --allow-read --allow-net=deno.land app1.ts json config prod app1 毎度実行する際に、上記のようなコマンドを打つのは辛いので、スクリプトランナーなるものを使うのが良さそう。Denoではvelociraptorが名前もそれっぽいので使いやすそう。\nちなみに、設定ファイルを指定するパスは スクリプトファイルから見た相対パスでなく、project_rootのディレクトリから見た相対パスであるので注意。今回は両者とも同じ場所だが、更にフォルダを作ってスクリプトと設定ファイルを入れた場合は、{作成したフォルダ名}/config と指定しなければならない。 上手く動かない際は、 const options: LoadConfigOptions = { verbose: false }; のオプションをtrueにするとログが出力されるのでオススメ。\n方法2. Deno標準機能だけで実装する サードパーティを使わない方法も試してみたので、こちらに記載。チュートリアルのexampleに載っている例を参考に、以下のようにjsonファイルを読み込む。2\nimport conf from \u0026#34;./config/config.json\u0026#34; assert { type: \u0026#34;json\u0026#34; }; console.log(conf); 単一のファイルを読み込む場合にはこちらのほうがシンプルだが、環境ごとに設定ファイルを分けたい場合などは結局引数から情報を渡す必要があるので、同じ感じにはなりそう。\nCompatibility with Node and npm\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nImporting JSON\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://ny1030.github.io/pages/posts/engineering/nodejs%E3%81%8B%E3%82%89deno%E3%81%B8%E3%81%AE%E5%BC%95%E8%B6%8A%E3%81%97/","summary":"※順次追記\nContext jsランタイム環境であるDenoが最近目にするので、勉強のためにNodejsで書いたコードをDenoに引越ししてみる。\nDeno: https://deno.land/\n処理別の実装方法 JSONのconfigをスクリプトに読み込む Nodejs node-config モジュールを使って以下のように、JSONファイルをオブジェクトとして読み込む。 const conf = require(\u0026#39;config\u0026#39;); //JSONファイルは ./config/default.json const INTERVAL = conf.interval; const LOWER_LIMIT = conf.lowerLimit; const UPPER_LIMIT = conf.upperLimit; Deno 最も簡単と思われる方法は以下のようにnpmをインポートする。 import config from \u0026#34;npm:config\u0026#34;; console.log(config); ただし、この機能自体はDenoが作られた経緯（脱npm）とは反するもので、Deno社も渋々npmをサポートしているように見える。1\nまた、npmのサポートは実験的な機能でまだサポートされてないパッケージもあるので、極力使わないほうがいいかもしれない。\n方法1. Denoで公開されているModuleを使う npmが使えない -\u0026gt; ならDenoで公開しているModuleを使う、という方法。Moduleは以下で公開されている。\nhttps://deno.land/x\nconfigで検索して出てきたModuleを使ってみる。Documentに書いてる例を参考に、以下のような構造のディレクトリを作成する。\n{project_root} ├── app1.ts └── config └── prod └── app1.json 設定ファイルが app1.json で、スクリプトが app1.ts とする。\nimport { Config, CONFIG_FORMATTERS, ConfigFormatter, loadConfig, LoadConfigOptions, } from \u0026#34;https://deno.land/x/load_config_files@0.3.0/mod.ts\u0026#34;; const options: LoadConfigOptions = { verbose: false }; const [formatterId, configRootPath, .","title":"NodeJSからDenoへの引越し"},{"content":"背景 AWSのEC2やRDSを使用するにあたり、インスタンスタイプを選定した上で使っているが、最近リリースされた新しめのインスタンス（この記事の時点だとr6iとか）を使う場合、特定のリージョンやAZで使えない場合がある。\nその場合の確認方法として、EC2であれば管理コンソールのEC2の画面から対応しているリージョン・AZを調べることが出来るが、RDSでは同様のインターフェースは現状提供されていない。\n確認方法 対応リージョン 対応リージョンについては、管理コンソールから確認できないものの、価格票のページから同等の情報を確認することが可能 1\nただし、AWS曰くまれに価格票が更新される際に使えるけど乗ってないケースがあるので、その際はリロードするなりしてくれとのこと。\n対応AZ 対応しているリージョンが分かった後に、そのリージョンで対応しているAZ（Availability Zone）を調べる場合は CLIコマンド describe-orderable-db-instance-options で確認が可能である 2\n例として、TokyoリージョンのAurora for PostgreSQL 14.3 の r6i.large が使えるAZを確認するコマンドは以下の通りである。\n$ aws rds describe-orderable-db-instance-options --region ap-northeast-1 --engine aurora-postgresql --engine-version 14.3 --db-instance-class db.r6i.large --query \u0026#39;OrderableDBInstanceOptions[].{EngineVersion:EngineVersion,DBInstanceClass:DBInstanceClass,AvailabilityZones:AvailabilityZones}\u0026#39; 出力結果は以下の通り。1c は現状では使えないみたい。\n[ { \u0026#34;EngineVersion\u0026#34;: \u0026#34;14.3\u0026#34;, \u0026#34;DBInstanceClass\u0026#34;: \u0026#34;db.r6i.large\u0026#34;, \u0026#34;AvailabilityZones\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;ap-northeast-1a\u0026#34; }, { \u0026#34;Name\u0026#34;: \u0026#34;ap-northeast-1d\u0026#34; } ] } ] https://aws.amazon.com/jp/rds/aurora/pricing/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.aws.amazon.com/cli/latest/reference/rds/describe-orderable-db-instance-options.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://ny1030.github.io/pages/posts/engineering/aws-aurora-%E3%81%A7-%E7%89%B9%E5%AE%9A%E3%81%AE%E3%82%A4%E3%83%B3%E3%82%B9%E3%82%BF%E3%83%B3%E3%82%B9%E3%82%BF%E3%82%A4%E3%83%97%E3%81%8C%E3%81%A9%E3%81%AEaz%E3%81%A7%E4%BD%BF%E3%81%88%E3%82%8B%E3%81%8B%E8%AA%BF%E3%81%B9%E3%82%8B%E6%96%B9%E6%B3%95/","summary":"背景 AWSのEC2やRDSを使用するにあたり、インスタンスタイプを選定した上で使っているが、最近リリースされた新しめのインスタンス（この記事の時点だとr6iとか）を使う場合、特定のリージョンやAZで使えない場合がある。\nその場合の確認方法として、EC2であれば管理コンソールのEC2の画面から対応しているリージョン・AZを調べることが出来るが、RDSでは同様のインターフェースは現状提供されていない。\n確認方法 対応リージョン 対応リージョンについては、管理コンソールから確認できないものの、価格票のページから同等の情報を確認することが可能 1\nただし、AWS曰くまれに価格票が更新される際に使えるけど乗ってないケースがあるので、その際はリロードするなりしてくれとのこと。\n対応AZ 対応しているリージョンが分かった後に、そのリージョンで対応しているAZ（Availability Zone）を調べる場合は CLIコマンド describe-orderable-db-instance-options で確認が可能である 2\n例として、TokyoリージョンのAurora for PostgreSQL 14.3 の r6i.large が使えるAZを確認するコマンドは以下の通りである。\n$ aws rds describe-orderable-db-instance-options --region ap-northeast-1 --engine aurora-postgresql --engine-version 14.3 --db-instance-class db.r6i.large --query \u0026#39;OrderableDBInstanceOptions[].{EngineVersion:EngineVersion,DBInstanceClass:DBInstanceClass,AvailabilityZones:AvailabilityZones}\u0026#39; 出力結果は以下の通り。1c は現状では使えないみたい。\n[ { \u0026#34;EngineVersion\u0026#34;: \u0026#34;14.3\u0026#34;, \u0026#34;DBInstanceClass\u0026#34;: \u0026#34;db.r6i.large\u0026#34;, \u0026#34;AvailabilityZones\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;ap-northeast-1a\u0026#34; }, { \u0026#34;Name\u0026#34;: \u0026#34;ap-northeast-1d\u0026#34; } ] } ] https://aws.amazon.com/jp/rds/aurora/pricing/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.aws.amazon.com/cli/latest/reference/rds/describe-orderable-db-instance-options.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;","title":"AWS RDS/Aurora で 特定のインスタンスタイプがどのAZで使えるか調べる方法"},{"content":" 特定のvolumeIdのSnapshot一覧を確認したい時 取得日時が新しい順に表示されてそう aws ec2 describe-snapshots --filters Name=\u0026#34;volume-id,Values=${volume}\u0026#34; describe-snapshots — AWS CLI 2.7.29 Command Reference\n","permalink":"https://ny1030.github.io/pages/posts/engineering/aws-cli-snapshot-commands/","summary":"特定のvolumeIdのSnapshot一覧を確認したい時 取得日時が新しい順に表示されてそう aws ec2 describe-snapshots --filters Name=\u0026#34;volume-id,Values=${volume}\u0026#34; describe-snapshots — AWS CLI 2.7.29 Command Reference","title":"AWS CLI Snapshot Commands"},{"content":"背景 Dockerにログインして、ディレクトリの構造などを確認したい場合、一般的なやり方だと以下のように起動しているコンテナのidを調べてログインをしている。\ndocker run {container_name} docker ps docker exec -it {container_id} /bin/sh こちらの方法だと、Dockerfileで指定しているコマンドが起動直後に終了する場合、docker ps してもコンテナが既に終了しており、ログイン出来ないという事がある。\n方法 docker run のオプションで -it を指定することで、コンテナが終了してもログイン出来るようになる。-it はそれぞれ独立したオプションで、-i(interactive) , -t(tty) 1\nコマンドとしては以下のような感じ：\ndocker run -it --entrypoint /bin/sh {container_name} Tips: container_name は docker images で確認。docker build -t {name}:{tag} でビルド時に名前を指定可能。\ndocker composeの場合 docker composeでも同様の事をしたい場合は、docker-compose.yamlで以下のオプションを追加すればよい。\ntty: true stdin_open: true * service.{service_name} の階層に書く Docker run リファレンス\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://ny1030.github.io/pages/posts/engineering/docker%E3%81%A7%E8%B5%B7%E5%8B%95%E3%81%97%E3%81%A6%E3%81%AA%E3%81%84%E3%82%B3%E3%83%B3%E3%83%86%E3%83%8A%E3%81%AB%E3%83%AD%E3%82%B0%E3%82%A4%E3%83%B3%E3%81%99%E3%82%8B%E6%96%B9%E6%B3%95/","summary":"背景 Dockerにログインして、ディレクトリの構造などを確認したい場合、一般的なやり方だと以下のように起動しているコンテナのidを調べてログインをしている。\ndocker run {container_name} docker ps docker exec -it {container_id} /bin/sh こちらの方法だと、Dockerfileで指定しているコマンドが起動直後に終了する場合、docker ps してもコンテナが既に終了しており、ログイン出来ないという事がある。\n方法 docker run のオプションで -it を指定することで、コンテナが終了してもログイン出来るようになる。-it はそれぞれ独立したオプションで、-i(interactive) , -t(tty) 1\nコマンドとしては以下のような感じ：\ndocker run -it --entrypoint /bin/sh {container_name} Tips: container_name は docker images で確認。docker build -t {name}:{tag} でビルド時に名前を指定可能。\ndocker composeの場合 docker composeでも同様の事をしたい場合は、docker-compose.yamlで以下のオプションを追加すればよい。\ntty: true stdin_open: true * service.{service_name} の階層に書く Docker run リファレンス\u0026#160;\u0026#x21a9;\u0026#xfe0e;","title":"Dockerで起動してないコンテナにログインする方法"},{"content":" データベース作成 curl -i -XPOST http://localhost:8086/query --data-urlencode \u0026#34;q=CREATE DATABASE mydb\u0026#34; Smapleデータ書き込み curl -i -XPOST \u0026#39;http://localhost:8086/write?db=mydb\u0026#39; --data-binary \u0026#39;cpu_load_short,host=server01,region=us-west value=0.64 1434055562000000000\u0026#39; 参考 Write data with the InfluxDB API\n","permalink":"https://ny1030.github.io/pages/posts/engineering/influxdb-%E9%96%A2%E9%80%A3%E3%82%B3%E3%83%9E%E3%83%B3%E3%83%89/","summary":"データベース作成 curl -i -XPOST http://localhost:8086/query --data-urlencode \u0026#34;q=CREATE DATABASE mydb\u0026#34; Smapleデータ書き込み curl -i -XPOST \u0026#39;http://localhost:8086/write?db=mydb\u0026#39; --data-binary \u0026#39;cpu_load_short,host=server01,region=us-west value=0.64 1434055562000000000\u0026#39; 参考 Write data with the InfluxDB API","title":"influxDB 関連コマンド"},{"content":"はじめに Hugo（静的サイトジェネレーター）でブログを書いてる際に、諸々の事情で公開したくないページが公開されてしまう事例があった。厳密にいうと、TOPページに出てくる記事の一覧からは見えないが、検索ボックスからそのページに含むキーワードを検索するとそのページが出てきてしまっていた。\nフォルダ構成の例 例として、Hugoの記事を格納しているフォルダ構成は以下の通り。\n. ├── archetypes └── content ├── posts │ └── engineering \u0026lt;--- 公開したい記事 ├── private. \u0026lt;--- ディレクトリ単位で公開したくない記事（今回の対象） └── template. │ └── template.md \u0026lt;--- ファイル単位で公開したくない記事（今回の対象） 今回は content/private のフォルダに入っている記事および content/template/template.md というファイルを公開したくないので、その設定方法を考える。\n設定方法 方法1. configファイルに指定 Hugoの設定ファイルである config.yaml に ignoreFiles ディレクティブを追加する。 ※ 設定ファイルの形式が TOMLの場合は config.toml に読み換えてください。\nignoreFiles: - content/template/template\\.md$ - content/private/.*$ 1つ目が content/template/template.md を公開しない設定でこれはほぼそのまま書けば良い。.（ドット）をエスケープ処理する必要があるため、\\（バックスラッシュ）を入れている。 2つ目がcontent/privateを公開しない設定であり、こちらは正規表現を使っている。ディレクトリ内のファイルを全て公開したくないので、 .* としている。\n上記設定により、検索しても記事が出てこないことを確認。記事のURLに直アクセスした場合も404が出ることを確認。\n方法2. frontmatterを使う Hugoで使える frontmatterを確認し、draft という変数を true にすれば下書きの記事として認識されるので、公開されない。 以下のように公開したくない記事の先頭に以下のスニペットを書けば良い。frontmatterはダッシュ（\u0026mdash;）も上下に書く必要があるので注意。\n--- draft: true --- 上記設定により、同様に検索しても記事が出てこないことを確認。\nどちらの設定方法が良いか 正規表現を使ってパターンに一致する複数の記事を非公開にしたい場合は方法1が良いと思う。一方で単体の記事を非公開にしたい場合はどちらの方法もメリット・デメリットがあると思った。\n方法1 メリット: どの記事が非公開設定されているかが単一のファイルに設定されている デメリット: 非公開にしたいファイルが増えてくると設定ファイルのボリュームが大きくなる。 方法2 メリット: 記事単位で非公開の設定ができるので、複数人で記事を書くようなユースケースに適している。 デメリット: 設定が複数ファイルに分散するのでどの記事が非公開になっているか調べるのが、方法1と比べると多少手間がかかる。 どちらでもよさそうだが、個人的には設定ファイルに可能な限りまとめたいので、方法1を使うことにした。ただ、方法1と方法2を両方使ってしまうと混乱を招きそうだなと思ったので、複数人でHugoの記事を書く場合はどちらかに統一するか認識を合わせる事が大事だなと思った。\n","permalink":"https://ny1030.github.io/pages/posts/engineering/hugo%E3%81%A7%E7%89%B9%E5%AE%9A%E3%81%AE%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%82%92%E5%85%AC%E9%96%8B%E3%81%97%E3%81%AA%E3%81%84%E6%96%B9%E6%B3%95/","summary":"はじめに Hugo（静的サイトジェネレーター）でブログを書いてる際に、諸々の事情で公開したくないページが公開されてしまう事例があった。厳密にいうと、TOPページに出てくる記事の一覧からは見えないが、検索ボックスからそのページに含むキーワードを検索するとそのページが出てきてしまっていた。\nフォルダ構成の例 例として、Hugoの記事を格納しているフォルダ構成は以下の通り。\n. ├── archetypes └── content ├── posts │ └── engineering \u0026lt;--- 公開したい記事 ├── private. \u0026lt;--- ディレクトリ単位で公開したくない記事（今回の対象） └── template. │ └── template.md \u0026lt;--- ファイル単位で公開したくない記事（今回の対象） 今回は content/private のフォルダに入っている記事および content/template/template.md というファイルを公開したくないので、その設定方法を考える。\n設定方法 方法1. configファイルに指定 Hugoの設定ファイルである config.yaml に ignoreFiles ディレクティブを追加する。 ※ 設定ファイルの形式が TOMLの場合は config.toml に読み換えてください。\nignoreFiles: - content/template/template\\.md$ - content/private/.*$ 1つ目が content/template/template.md を公開しない設定でこれはほぼそのまま書けば良い。.（ドット）をエスケープ処理する必要があるため、\\（バックスラッシュ）を入れている。 2つ目がcontent/privateを公開しない設定であり、こちらは正規表現を使っている。ディレクトリ内のファイルを全て公開したくないので、 .* としている。\n上記設定により、検索しても記事が出てこないことを確認。記事のURLに直アクセスした場合も404が出ることを確認。\n方法2. frontmatterを使う Hugoで使える frontmatterを確認し、draft という変数を true にすれば下書きの記事として認識されるので、公開されない。 以下のように公開したくない記事の先頭に以下のスニペットを書けば良い。frontmatterはダッシュ（\u0026mdash;）も上下に書く必要があるので注意。\n--- draft: true --- 上記設定により、同様に検索しても記事が出てこないことを確認。","title":"Hugoで特定のファイルを公開しない方法"},{"content":"参考：Generate And Install A Let\u0026rsquo;s Encrypt SSL Certificate For A Bitnami Application\n基本は以下のbitnami提供のToolを実行するだけでOK\nsudo /opt/bitnami/bncert-tool 対話形式で以下の事項が聞かれるので回答\nサイトのドメイン名 このタイミングでAレコードに上記ドメイン \u0026lt;-\u0026gt; 当該ホストのIP が設定されてないと、名前解決できないというエラーが出るので事前にDNSレコード（Aレコード）を設定しておくのがベター wwwドメインを設定するか メールアドレス 最終確認 ","permalink":"https://ny1030.github.io/pages/posts/engineering/bitnami%E3%81%A7%E6%A7%8B%E7%AF%89%E3%81%97%E3%81%9F%E3%82%B5%E3%82%A4%E3%83%88%E3%81%ABletsencrypt%E3%82%92%E5%B0%8E%E5%85%A5/","summary":"参考：Generate And Install A Let\u0026rsquo;s Encrypt SSL Certificate For A Bitnami Application\n基本は以下のbitnami提供のToolを実行するだけでOK\nsudo /opt/bitnami/bncert-tool 対話形式で以下の事項が聞かれるので回答\nサイトのドメイン名 このタイミングでAレコードに上記ドメイン \u0026lt;-\u0026gt; 当該ホストのIP が設定されてないと、名前解決できないというエラーが出るので事前にDNSレコード（Aレコード）を設定しておくのがベター wwwドメインを設定するか メールアドレス 最終確認 ","title":"Bitnamiで構築したサイトにletsencryptを導入する手順"},{"content":"PHP-FPM Process Caluculator - Chris Moore を用いて計算するのが良い。 vCPU: 1, Memory: 1GBだと以下で設定している：\npm=static pm.max_children=3 pm.max_requests=100 ","permalink":"https://ny1030.github.io/pages/posts/engineering/php-fpm%E3%81%AE%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF/","summary":"PHP-FPM Process Caluculator - Chris Moore を用いて計算するのが良い。 vCPU: 1, Memory: 1GBだと以下で設定している：\npm=static pm.max_children=3 pm.max_requests=100 ","title":"php-fpmのパラメータ設定"},{"content":"前提 Apple Developer アカウントは作成済み Xcodeをインストールした端末（MacBookなど）が手元にある [Xcode] Xcodeプロジェクトの設定 {project_root}/ios/Runner.xcodeproj をXcodeで開き、Auto-Signningやアイコンの画像など諸々の設定を実施する。基本は以下のFlutterドキュメントに倣って設定すれば良い。 https://docs.flutter.dev/deployment/ios#review-xcode-project-settings [Flutter] ビルドの実施 {project_root}で以下を実施する。 flutter clean flutter build ipa -\u0026gt; 上記の結果として、build/ios/archive/Runner.xcarchive が出力される。\n[Xcode] TestFlightに配信 build/ios/archive/Runner.xcarchive をXcodeで開く Distribute App -\u0026gt; App Store Connect -\u0026gt; Upload を順にクリック 以下はデフォルトのままの設定にしている Review画面が出てくるので、確認しUploadをクリックし暫く待つ [Browser] TestFlightで任意のグループやユーザにアプリを配信 https://appstoreconnect.apple.com/ を開き、My Apps -\u0026gt; Test Flightへ 以下のように +ボタンから任意のグループやユーザを追加する -\u0026gt; 追加されると以下のようなメールが各ユーザに届くので、メールのLinkからアプリをDownloadする。 {APP_NAME} for iOS is now available to test. Tips App Store Connect でアプリの申請をするときにアプリのスクショをiPhone, iPad用それぞれで提出する必要がある。画像の解像度が各要件を満たしてないとアップロードできないので、とりあえずアップロードしたい場合は、以下のImagemagicのコマンドでオリジナル画像をリサイズすることでアップロードできた。\n*前提として、変換する画像は４枚（IMG0[1~4].PNG）\n#iPhone 6.5inch用 convert -resize 1284x2778! IMG01.PNG 6_5in_01.png convert -resize 1284x2778! IMG02.PNG 6_5in_02.png convert -resize 1284x2778! IMG03.PNG 6_5in_03.png convert -resize 1284x2778! IMG04.PNG 6_5in_04.png #iPhone 5.5inch用 convert -resize 1242x2208! IMG01.PNG 5_5in_01.png convert -resize 1242x2208! IMG02.PNG 5_5in_02.png convert -resize 1242x2208! IMG03.PNG 5_5in_03.png convert -resize 1242x2208! IMG04.PNG 5_5in_04.png #iPad 12.9inch用 convert -resize 2048x2732! IMG01.PNG 12_9in_01.png convert -resize 2048x2732! IMG02.PNG 12_9in_02.png convert -resize 2048x2732! IMG03.PNG 12_9in_03.png convert -resize 2048x2732! IMG04.PNG 12_9in_04.png ","permalink":"https://ny1030.github.io/pages/posts/engineering/flutter%E3%81%A7ios%E3%82%A2%E3%83%97%E3%83%AA%E3%82%92testflight%E3%81%A7%E3%83%AA%E3%83%AA%E3%83%BC%E3%82%B9%E3%81%99%E3%82%8B%E6%89%8B%E9%A0%86/","summary":"前提 Apple Developer アカウントは作成済み Xcodeをインストールした端末（MacBookなど）が手元にある [Xcode] Xcodeプロジェクトの設定 {project_root}/ios/Runner.xcodeproj をXcodeで開き、Auto-Signningやアイコンの画像など諸々の設定を実施する。基本は以下のFlutterドキュメントに倣って設定すれば良い。 https://docs.flutter.dev/deployment/ios#review-xcode-project-settings [Flutter] ビルドの実施 {project_root}で以下を実施する。 flutter clean flutter build ipa -\u0026gt; 上記の結果として、build/ios/archive/Runner.xcarchive が出力される。\n[Xcode] TestFlightに配信 build/ios/archive/Runner.xcarchive をXcodeで開く Distribute App -\u0026gt; App Store Connect -\u0026gt; Upload を順にクリック 以下はデフォルトのままの設定にしている Review画面が出てくるので、確認しUploadをクリックし暫く待つ [Browser] TestFlightで任意のグループやユーザにアプリを配信 https://appstoreconnect.apple.com/ を開き、My Apps -\u0026gt; Test Flightへ 以下のように +ボタンから任意のグループやユーザを追加する -\u0026gt; 追加されると以下のようなメールが各ユーザに届くので、メールのLinkからアプリをDownloadする。 {APP_NAME} for iOS is now available to test. Tips App Store Connect でアプリの申請をするときにアプリのスクショをiPhone, iPad用それぞれで提出する必要がある。画像の解像度が各要件を満たしてないとアップロードできないので、とりあえずアップロードしたい場合は、以下のImagemagicのコマンドでオリジナル画像をリサイズすることでアップロードできた。\n*前提として、変換する画像は４枚（IMG0[1~4].PNG）\n#iPhone 6.5inch用 convert -resize 1284x2778! IMG01.PNG 6_5in_01.png convert -resize 1284x2778!","title":"FlutterでiOSアプリをTestFlightでリリースする手順"},{"content":"1. 記録するログファイルを作成 今回は例として、 /tmp にslow.log を作成し、このファイルにスロークエリを記録するようにする。\nファイル作成\u0026amp;権限付与： touch /tmp/slow.log sudo chown mysql:mysql -R /tmp/slow.log 権限確認 ls -l output: -rw-r--r-- 1 mysql mysql 451 Jun 22 17:21 slow.log -\u0026gt; ownerがmysqlになっている\n2. スロークエリを出力する設定を有効化 MySQLの設定ファイルである my.cnf にスロークエリを出力する設定を入れる。今回は５秒以上のクエリを出力する。 /etc/my.cnf にあるファイルを開き、以下を記載：\n[mysqld] ... long_query_time=5 slow_query_log_file=/tmp/slow.log slow_query_log=1 ... 記載できたらMySQLを再起動\nservice restart mysqld 3.確認 MySQLにログインし、以下のクエリを実行\nSELECT SLEEP(5); /tmp/slow.logを確認すると、無事にスロークエリとして出力されていた。\n... # Time: 220622 17:21:43 # User@Host: root[root] @ localhost [] # Thread_id: 118 Schema: QC_hit: No # Query_time: 5.000241 Lock_time: 0.000000 Rows_sent: 1 Rows_examined: 0 # Rows_affected: 0 Bytes_sent: 63 SET timestamp=1655918503; SELECT SLEEP(5); ... ","permalink":"https://ny1030.github.io/pages/posts/engineering/mysql-mariddb%E3%81%A7%E3%82%B9%E3%83%AD%E3%83%BC%E3%82%AF%E3%82%A8%E3%83%AA%E3%82%92%E5%87%BA%E5%8A%9B%E3%81%99%E3%82%8B%E8%A8%AD%E5%AE%9A/","summary":"1. 記録するログファイルを作成 今回は例として、 /tmp にslow.log を作成し、このファイルにスロークエリを記録するようにする。\nファイル作成\u0026amp;権限付与： touch /tmp/slow.log sudo chown mysql:mysql -R /tmp/slow.log 権限確認 ls -l output: -rw-r--r-- 1 mysql mysql 451 Jun 22 17:21 slow.log -\u0026gt; ownerがmysqlになっている\n2. スロークエリを出力する設定を有効化 MySQLの設定ファイルである my.cnf にスロークエリを出力する設定を入れる。今回は５秒以上のクエリを出力する。 /etc/my.cnf にあるファイルを開き、以下を記載：\n[mysqld] ... long_query_time=5 slow_query_log_file=/tmp/slow.log slow_query_log=1 ... 記載できたらMySQLを再起動\nservice restart mysqld 3.確認 MySQLにログインし、以下のクエリを実行\nSELECT SLEEP(5); /tmp/slow.logを確認すると、無事にスロークエリとして出力されていた。\n... # Time: 220622 17:21:43 # User@Host: root[root] @ localhost [] # Thread_id: 118 Schema: QC_hit: No # Query_time: 5.","title":"MySQL, MaridDBでスロークエリを出力する設定"},{"content":"やりたいこと Spring Boot で オンラインアプリケーションを作成した時に、DBとの接続をコネクションプール（以降コネプ）を用いる時に、コネプのサイズが適正かどうかを確認するためにログに使用状況を出したい。 *前提として、使用するコネプライブラリはSpring Boot 2.0のデフォルトであるHikariCPとする。\n設定方法の例 logback.xmlに以下のようなloggerディレクティブを追加する。\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;appender name=\u0026#34;STDOUT\u0026#34; class=\u0026#34;ch.qos.logback.core.ConsoleAppender\u0026#34;\u0026gt; \u0026lt;encoder\u0026gt; \u0026lt;pattern\u0026gt;%msg%n\u0026lt;/pattern\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;logger name=\u0026#34;com.zaxxer.hikari\u0026#34; level=\u0026#34;DEBUG\u0026#34; additivity=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;appender-ref ref=\u0026#34;STDOUT\u0026#34;/\u0026gt; \u0026lt;/logger\u0026gt; \u0026lt;root level=\u0026#34;INFO\u0026#34;\u0026gt; \u0026lt;appender-ref ref=\u0026#34;STDOUT\u0026#34; /\u0026gt; \u0026lt;/root\u0026gt; \u0026lt;/configuration\u0026gt; 上記設定をおこなうことで、以下のようなコネプ使用状況のログが30秒間隔で出力されることを確認。\n... 2020-06-04T02:26:18+00:00 serviceA.xxxxxx.debug {\u0026#34;hostName\u0026#34;:\u0026#34;xxxxxx\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;DEBUG\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2020-06-04T02:26:18.473763Z\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;HikariPool-2 - Pool stats (total=10, active=0, idle=10, waiting=0)\u0026#34;} 2020-06-04T02:26:47+00:00 serviceA.xxxxxx.debug {\u0026#34;hostName\u0026#34;:\u0026#34;xxxxxx\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;DEBUG\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2020-06-04T02:26:47.881731Z\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;HikariPool-1 - Pool stats (total=20, active=0, idle=20, waiting=0)\u0026#34;} 2020-06-04T02:26:48+00:00 serviceA.xxxxxx.debug {\u0026#34;hostName\u0026#34;:\u0026#34;xxxxxx\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;DEBUG\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2020-06-04T02:26:48.473995Z\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;HikariPool-2 - Pool stats (total=10, active=0, idle=10, waiting=0)\u0026#34;} ... messageに出力されている内容を抜粋すると以下の通り。\nHikariPool-2 - Pool stats (total=10, active=0, idle=10, waiting=0) それぞれ説明すると、\nHikariPool-2：コネプで設定しているJDBCのデータソース名。今回はWriterとReaderでそれぞれデータソースを設定したので、1と2がそれぞれ出ている。 Pool stats：コネプの利用状況 total：確保しているpoolの最大数。 active：現在利用されているpoolの数。 idle：total - active の数。つまり待機状態のpool数。 waiting：poolを割り当てるのを待っているタスクの数？ ","permalink":"https://ny1030.github.io/pages/posts/engineering/spring-boot-%E3%81%AEjdbc%E3%82%B3%E3%83%8D%E3%82%AF%E3%82%B7%E3%83%A7%E3%83%B3%E3%83%97%E3%83%BC%E3%83%ABhikaricp%E3%81%AE%E3%83%AD%E3%82%AE%E3%83%B3%E3%82%B0%E6%96%B9%E6%B3%95/","summary":"やりたいこと Spring Boot で オンラインアプリケーションを作成した時に、DBとの接続をコネクションプール（以降コネプ）を用いる時に、コネプのサイズが適正かどうかを確認するためにログに使用状況を出したい。 *前提として、使用するコネプライブラリはSpring Boot 2.0のデフォルトであるHikariCPとする。\n設定方法の例 logback.xmlに以下のようなloggerディレクティブを追加する。\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;appender name=\u0026#34;STDOUT\u0026#34; class=\u0026#34;ch.qos.logback.core.ConsoleAppender\u0026#34;\u0026gt; \u0026lt;encoder\u0026gt; \u0026lt;pattern\u0026gt;%msg%n\u0026lt;/pattern\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;logger name=\u0026#34;com.zaxxer.hikari\u0026#34; level=\u0026#34;DEBUG\u0026#34; additivity=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;appender-ref ref=\u0026#34;STDOUT\u0026#34;/\u0026gt; \u0026lt;/logger\u0026gt; \u0026lt;root level=\u0026#34;INFO\u0026#34;\u0026gt; \u0026lt;appender-ref ref=\u0026#34;STDOUT\u0026#34; /\u0026gt; \u0026lt;/root\u0026gt; \u0026lt;/configuration\u0026gt; 上記設定をおこなうことで、以下のようなコネプ使用状況のログが30秒間隔で出力されることを確認。\n... 2020-06-04T02:26:18+00:00 serviceA.xxxxxx.debug {\u0026#34;hostName\u0026#34;:\u0026#34;xxxxxx\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;DEBUG\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2020-06-04T02:26:18.473763Z\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;HikariPool-2 - Pool stats (total=10, active=0, idle=10, waiting=0)\u0026#34;} 2020-06-04T02:26:47+00:00 serviceA.xxxxxx.debug {\u0026#34;hostName\u0026#34;:\u0026#34;xxxxxx\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;DEBUG\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2020-06-04T02:26:47.881731Z\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;HikariPool-1 - Pool stats (total=20, active=0, idle=20, waiting=0)\u0026#34;} 2020-06-04T02:26:48+00:00 serviceA.xxxxxx.debug {\u0026#34;hostName\u0026#34;:\u0026#34;xxxxxx\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;DEBUG\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2020-06-04T02:26:48.473995Z\u0026#34;,\u0026#34;message\u0026#34;:\u0026#34;HikariPool-2 - Pool stats (total=10, active=0, idle=10, waiting=0)\u0026#34;} ... messageに出力されている内容を抜粋すると以下の通り。\nHikariPool-2 - Pool stats (total=10, active=0, idle=10, waiting=0) それぞれ説明すると、","title":"Spring Boot のJDBCコネクションプール（HikariCP）のロギング方法"},{"content":" インスタンス情報をCSV出力（ヘッダなし） aws rds describe-db-instances --query \u0026#34;DBInstances[].[DBInstanceIdentifier,DBInstanceClass,Engine,EngineVersion]\u0026#34; | jq -r \u0026#34;.[] | @csv\u0026#34; \u0026gt; rds_describe-db-instances.csv インスタンス情報をCSV出力（ヘッダあり） aws rds describe-db-instances --query \u0026#34;DBInstances[].[DBInstanceIdentifier,DBInstanceClass,Engine,EngineVersion]\u0026#34; | jq -r \u0026#39;[\u0026#34;DBInstanceIdentifier\u0026#34;,\u0026#34;DBInstanceClass\u0026#34;,\u0026#34;Engine\u0026#34;,\u0026#34;EngineVersion\u0026#34;],(.[])|@csv\u0026#39; \u0026gt; rds_describe-db-instances_with-header.csv ","permalink":"https://ny1030.github.io/pages/posts/engineering/rds-cli/","summary":" インスタンス情報をCSV出力（ヘッダなし） aws rds describe-db-instances --query \u0026#34;DBInstances[].[DBInstanceIdentifier,DBInstanceClass,Engine,EngineVersion]\u0026#34; | jq -r \u0026#34;.[] | @csv\u0026#34; \u0026gt; rds_describe-db-instances.csv インスタンス情報をCSV出力（ヘッダあり） aws rds describe-db-instances --query \u0026#34;DBInstances[].[DBInstanceIdentifier,DBInstanceClass,Engine,EngineVersion]\u0026#34; | jq -r \u0026#39;[\u0026#34;DBInstanceIdentifier\u0026#34;,\u0026#34;DBInstanceClass\u0026#34;,\u0026#34;Engine\u0026#34;,\u0026#34;EngineVersion\u0026#34;],(.[])|@csv\u0026#39; \u0026gt; rds_describe-db-instances_with-header.csv ","title":"RDS CLI"},{"content":"Python 以下のような引数付きのPythonスクリプトをVSCode上のデバッグモードで実行する場合\nコマンド python 03_filter.py ../data/1653823316.json VSCode上の設定（launch.json） { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Launch Python\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;03_filter.py\u0026#34;, \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}/app/script\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;python\u0026#34;, \u0026#34;args\u0026#34;: [\u0026#34;../data/1653823316.json\u0026#34;] } ] } 上記のような設定をすることで動くことを確認。補足として、cwd (change work directory) を入れないと、実行した際に ../data/1653823316.json not found みたいなエラーが出たので、相対パスでファイルを読み込むコマンドを実行する時は入れておいたほうがいい。\nNodejs コマンド node 11_post.js ../data/1655618612.json VSCode上の設定（launch.json） { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Launch Node (11_post.js)\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;11_post.js\u0026#34;, \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}/app/script\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;args\u0026#34;: [\u0026#34;../data/1655618612.json\u0026#34;], \u0026#34;runtimeExecutable\u0026#34;: \u0026#34;/usr/local/bin/node\u0026#34;, \u0026#34;skipFiles\u0026#34;: [ \u0026#34;\u0026lt;node_internals\u0026gt;/**\u0026#34; ], \u0026#34;type\u0026#34;: \u0026#34;node\u0026#34; } ] } ","permalink":"https://ny1030.github.io/pages/posts/engineering/vscode%E3%81%A7%E3%81%AE%E3%83%87%E3%83%90%E3%83%83%E3%82%B0%E3%83%A2%E3%83%BC%E3%83%89%E7%94%A8%E3%81%AE%E8%A8%AD%E5%AE%9A%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E4%BE%8B/","summary":"Python 以下のような引数付きのPythonスクリプトをVSCode上のデバッグモードで実行する場合\nコマンド python 03_filter.py ../data/1653823316.json VSCode上の設定（launch.json） { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Launch Python\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;03_filter.py\u0026#34;, \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}/app/script\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;python\u0026#34;, \u0026#34;args\u0026#34;: [\u0026#34;../data/1653823316.json\u0026#34;] } ] } 上記のような設定をすることで動くことを確認。補足として、cwd (change work directory) を入れないと、実行した際に ../data/1653823316.json not found みたいなエラーが出たので、相対パスでファイルを読み込むコマンドを実行する時は入れておいたほうがいい。\nNodejs コマンド node 11_post.js ../data/1655618612.json VSCode上の設定（launch.json） { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Launch Node (11_post.js)\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;11_post.js\u0026#34;, \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}/app/script\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;args\u0026#34;: [\u0026#34;../data/1655618612.json\u0026#34;], \u0026#34;runtimeExecutable\u0026#34;: \u0026#34;/usr/local/bin/node\u0026#34;, \u0026#34;skipFiles\u0026#34;: [ \u0026#34;\u0026lt;node_internals\u0026gt;/**\u0026#34; ], \u0026#34;type\u0026#34;: \u0026#34;node\u0026#34; } ] } ","title":"VSCodeで引数付きのプログラムをデバッグモードで動かす時の設定例"},{"content":"CSVファイルを読み込む let datas = fs.readFileSync(\u0026#39;data/chart/1m_\u0026#39; + code + \u0026#39;.csv\u0026#39;).toString().split(\u0026#34;\\r\\n\u0026#34;); ","permalink":"https://ny1030.github.io/pages/posts/engineering/nodejs%E3%81%A7csv%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%82%92%E8%AA%AD%E3%81%BF%E8%BE%BC%E3%82%93%E3%81%A0%E3%82%8A%E6%9B%B8%E3%81%8D%E5%87%BA%E3%81%97%E3%81%9F%E3%82%8A%E3%81%99%E3%82%8B/","summary":"CSVファイルを読み込む let datas = fs.readFileSync(\u0026#39;data/chart/1m_\u0026#39; + code + \u0026#39;.csv\u0026#39;).toString().split(\u0026#34;\\r\\n\u0026#34;); ","title":"nodejsでCSVファイルを作成する"},{"content":" 特定のパラメータの値をチェック ","permalink":"https://ny1030.github.io/pages/posts/engineering/rds%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E7%A2%BA%E8%AA%8Dcli/","summary":" 特定のパラメータの値をチェック ","title":"RDSパラメータ確認CLI"},{"content":"単発実行 AMI 現在のインスタンスのTagの状態を確認 aws ec2 describe-images --image-ids ami-xxxxxx タグを付与(例として system:front というTagを付与する) aws ec2 create-tags --resources ami-ffa5df99 --tags Key=system,Value=front ⇨再度実行しても同じKey/Valueなら結果は変わらないことを確認\nsystemのタグが付いてるリソースを列挙 aws ec2 describe-images --filter Name=\u0026#34;tag-key\u0026#34;,Values=\u0026#34;system\u0026#34; ワイルドカードによる検索も可能 aws ec2 describe-images --filter Name=\u0026#34;tag-key\u0026#34;,Values=\u0026#34;sys*\u0026#34; タグのKeyが system の値を取得 aws ec2 describe-images --image-ids ami-xxxxxx | jq \u0026#39;.Snapshots[] | [ .Tags[] | select(.Key == \u0026#34;system\u0026#34;).Value]\u0026#39; Snapshot 現在のTagの状態を確認\nsnapshotIdをキーに確認する場合 VolumeId=$(aws ec2 describe-snapshots --snapshot-ids snap-xxxxxx | jq .Snapshots[].VolumeId | tr -d \u0026#39;\u0026#34;\u0026#39;) aws ec2 describe-volumes --volume-ids $VolumeId AMIのImageIdをキーに確認する場合 SnapshotId=$(aws ec2 describe-images --image-ids ami-xxxxxxx | jq .Images[].BlockDeviceMappings[].Ebs.SnapshotId | tr -d \u0026#39;\u0026#34;\u0026#39;) VolumeId=$(aws ec2 describe-snapshots --snapshot-ids ${SnapshotId} | jq .Snapshots[].VolumeId | tr -d \u0026#39;\u0026#34;\u0026#39;) aws ec2 describe-volumes --volume-ids $VolumeId タグを付与 -\u0026gt; AMIと同じ。 resource に指定するIDをsnapshotIdにすればよい。\nReference describe-instances — AWS CLI 1.25.2 Command Reference describe-images — AWS CLI 1.25.4 Command Reference\n","permalink":"https://ny1030.github.io/pages/posts/engineering/ec2%E3%82%A4%E3%83%B3%E3%82%B9%E3%82%BF%E3%83%B3%E3%82%B9%E3%81%AB%E3%82%BF%E3%82%B0%E3%82%92%E4%BB%98%E4%B8%8E/","summary":"単発実行 AMI 現在のインスタンスのTagの状態を確認 aws ec2 describe-images --image-ids ami-xxxxxx タグを付与(例として system:front というTagを付与する) aws ec2 create-tags --resources ami-ffa5df99 --tags Key=system,Value=front ⇨再度実行しても同じKey/Valueなら結果は変わらないことを確認\nsystemのタグが付いてるリソースを列挙 aws ec2 describe-images --filter Name=\u0026#34;tag-key\u0026#34;,Values=\u0026#34;system\u0026#34; ワイルドカードによる検索も可能 aws ec2 describe-images --filter Name=\u0026#34;tag-key\u0026#34;,Values=\u0026#34;sys*\u0026#34; タグのKeyが system の値を取得 aws ec2 describe-images --image-ids ami-xxxxxx | jq \u0026#39;.Snapshots[] | [ .Tags[] | select(.Key == \u0026#34;system\u0026#34;).Value]\u0026#39; Snapshot 現在のTagの状態を確認\nsnapshotIdをキーに確認する場合 VolumeId=$(aws ec2 describe-snapshots --snapshot-ids snap-xxxxxx | jq .Snapshots[].VolumeId | tr -d \u0026#39;\u0026#34;\u0026#39;) aws ec2 describe-volumes --volume-ids $VolumeId AMIのImageIdをキーに確認する場合 SnapshotId=$(aws ec2 describe-images --image-ids ami-xxxxxxx | jq .","title":"EC2インスタンス/AMI/Snapshotにタグを付与"},{"content":"基本コンセプト Composition コンポジションはインフラモジュールの集合体であり、論理的に分離された複数の領域（例：AWSリージョン、複数のAWSアカウント）にまたがることが可能である。コンポジションは、組織全体やプロジェクトに必要な完全なインフラストラクチャを表現するために使用される。\nコンポジションは、インフラストラクチャーモジュールで構成され、リソースモジュールで構成され、個々のリソースを実装する。 作成すべきファイル main.tf: モジュール、ローカル、データソースを呼び出して、すべてのリソースを作成 outpputs.tf: main.tfで作成されたリソースからの出力 variables.tf: main.tfで使用される変数の宣言 terraform.tfvars: 環境特有の変数の値を宣言 ref: https://github.com/antonbabenko/terraform-best-practices/tree/master/examples/large-terraform/stage\n","permalink":"https://ny1030.github.io/pages/posts/engineering/terraform%E3%81%AE%E3%81%8A%E4%BD%9C%E6%B3%95/","summary":"基本コンセプト Composition コンポジションはインフラモジュールの集合体であり、論理的に分離された複数の領域（例：AWSリージョン、複数のAWSアカウント）にまたがることが可能である。コンポジションは、組織全体やプロジェクトに必要な完全なインフラストラクチャを表現するために使用される。\nコンポジションは、インフラストラクチャーモジュールで構成され、リソースモジュールで構成され、個々のリソースを実装する。 作成すべきファイル main.tf: モジュール、ローカル、データソースを呼び出して、すべてのリソースを作成 outpputs.tf: main.tfで作成されたリソースからの出力 variables.tf: main.tfで使用される変数の宣言 terraform.tfvars: 環境特有の変数の値を宣言 ref: https://github.com/antonbabenko/terraform-best-practices/tree/master/examples/large-terraform/stage","title":"Terraformのお作法"},{"content":"概要 ApacheやNginx、ELBのアクセスログの集計などをする時に以下のような変換を行う必要がある。\nInput:\ntimestamp,backend_processing_time_msec,alb_status_code,backend_status_code,target_status_code_list,method,URL 2021-12-01T03:50:00.115676Z,182,201,201,201,POST,https://api.test.io:443/service1/v2/jp/cart/7219b08ec8464865a6020bb6025cd641/details 2021-12-01T03:50:20.597508Z,67,200,200,200,GET,https://api.test.io:443/service2/v2/jp/history/0130050002112010350-8052922 2021-12-01T03:50:20.613452Z,145,200,200,200,GET,https://api.test.io:443/service2/v2/jp/history?display_results=5\u0026amp;search_page=1 2021-12-01T03:50:20.894114Z,22,200,200,200,GET,https://api.test.io:443/service2/v2/jp/history/0130050002112010350-8052921 2021-12-01T03:51:45.903017Z,8,404,404,404,DELETE,https://api.test.io:443/service3/v1/jp/reserve/7041b995fa1b4c8b99543acc50c60865 2021-12-01T03:54:41.598315Z,20,200,200,200,GET,https://api.test.io:443/service3/v1/jp/stocks?code_list=11111111%22222222%33333333 2021-12-01T03:54:56.672346Z,165,200,200,200,GET,https://api.test.io:443/service4/v1/jp/pay/accesstoken?device_id=AAAAA-BBBB-CCC ⬇️ URLの正規化（≒変換, 名寄せ）\nOutput:\ntimestamp,backend_processing_time_msec,alb_status_code,backend_status_code,method,URL 2021-12-01T03:50:00.115676Z,182.0,201,201,POST,https://api.test.io:443/service1/v2/jp/cart/{cart_no}/details 2021-12-01T03:50:20.597508Z,67.0,200,200,GET,https://api.test.io:443/service2/v2/jp/history/{order_no} 2021-12-01T03:50:20.613452Z,145.0,200,200,GET,https://api.test.io:443/service2/v2/jp/history 2021-12-01T03:50:20.894114Z,22.0,200,200,GET,https://api.test.io:443/service2/v2/jp/history/{order_no} 2021-12-01T03:51:45.903017Z,8.0,404,404,DELETE,https://api.test.io:443/service3/v1/jp/reserve/{cart_no} 2021-12-01T03:54:41.598315Z,20.0,200,200,GET,https://api.test.io:443/service3/v1/jp/stocks 2021-12-01T03:54:56.672346Z,165.0,200,200,GET,https://api.test.io:443/service4/v1/jp/pay/accesstoken 上記のような変換を行うことで、APIのEndpointごとにコール回数やレスポンス時間など統計処理をPandasなどを行うことができる。\nコード 概要で説明したような事を実現するために、以下のコードを作成。\n1. URLの変換パターンを定義した設定ファイル（JSON） 置換のパターンはAPIの定義書などを参考に記述するのが良い（ない場合はアクセスログからリバースするしかない\u0026hellip;） { \u0026#34;^(.*)/history/[0-9]{12,19}-[0-9]{5,7}\u0026#34;: \u0026#34;\\\\1/history/{order_no}\u0026#34;, \u0026#34;^(.*)(cart|reserve)/[0-9a-z]{32}\u0026#34;: \u0026#34;\\\\1\\\\2/{cart_no}\u0026#34; } 2. inputしたCSVからURLの変換処理を行うスクリプト（Python） タイトル詐欺になるが、クエリパラメータに関する置換はルールが単純なのでJSONでは定義せず、こちらのスクリプトで定義・処理している（# REMOVE QUERY PARAMETERのセクション）。 import pandas as pd import json input_filename = \u0026#39;./input.csv\u0026#39; output_filename = \u0026#39;./output.csv\u0026#39; input_csv = pd.read_csv( input_filename, sep=\u0026#39;,\u0026#39;, usecols=lambda x: x in [\u0026#39;timestamp\u0026#39;,\u0026#39;alb_status_code\u0026#39;, \u0026#39;backend_status_code\u0026#39;, \u0026#39;method\u0026#39;, \u0026#39;URL\u0026#39;, \u0026#39;backend_processing_time_msec\u0026#39;], index_col=\u0026#39;timestamp\u0026#39;, dtype={\u0026#39;timestamp\u0026#39;: str, \u0026#39;alb_status_code\u0026#39;: str, \u0026#39;backend_status_code\u0026#39;: str, \u0026#39;method\u0026#39;: str, \u0026#39;URL\u0026#39;: str, \u0026#39;backend_processing_time_msec\u0026#39;: \u0026#39;float16\u0026#39;} ) # REMOVE QUERY PARAMETER input_csv = pd.concat([input_csv, input_csv[\u0026#34;URL\u0026#34;].str.extract(r\u0026#39;([^\\?]+).*\u0026#39;, expand=True)], axis=1, ignore_index=False) input_csv = input_csv.drop(columns=[\u0026#34;URL\u0026#34;]) input_csv.rename(columns={0: \u0026#39;URL\u0026#39;}, inplace=True) # REPLACE ID IN URL TEXT FOR AGGREGATION with open(\u0026#39;./replace-pattern.json\u0026#39;) as f: dct = json.load(f) input_csv[\u0026#34;URL\u0026#34;] = input_csv[\u0026#34;URL\u0026#34;].replace(dct, regex=True) # Output to CSV input_csv.to_csv(output_filename, header=True, index=True) exit python test.py 実行することで概要通りの csvに変換されることを確認。\n","permalink":"https://ny1030.github.io/pages/posts/engineering/json%E3%81%A7%E5%AE%9A%E7%BE%A9%E3%81%97%E3%81%9Furl%E3%81%AE%E7%BD%AE%E6%8F%9B%E3%83%AB%E3%83%BC%E3%83%AB%E3%82%92python%E3%81%A7%E5%A4%89%E6%8F%9B%E3%81%99%E3%82%8B/","summary":"概要 ApacheやNginx、ELBのアクセスログの集計などをする時に以下のような変換を行う必要がある。\nInput:\ntimestamp,backend_processing_time_msec,alb_status_code,backend_status_code,target_status_code_list,method,URL 2021-12-01T03:50:00.115676Z,182,201,201,201,POST,https://api.test.io:443/service1/v2/jp/cart/7219b08ec8464865a6020bb6025cd641/details 2021-12-01T03:50:20.597508Z,67,200,200,200,GET,https://api.test.io:443/service2/v2/jp/history/0130050002112010350-8052922 2021-12-01T03:50:20.613452Z,145,200,200,200,GET,https://api.test.io:443/service2/v2/jp/history?display_results=5\u0026amp;search_page=1 2021-12-01T03:50:20.894114Z,22,200,200,200,GET,https://api.test.io:443/service2/v2/jp/history/0130050002112010350-8052921 2021-12-01T03:51:45.903017Z,8,404,404,404,DELETE,https://api.test.io:443/service3/v1/jp/reserve/7041b995fa1b4c8b99543acc50c60865 2021-12-01T03:54:41.598315Z,20,200,200,200,GET,https://api.test.io:443/service3/v1/jp/stocks?code_list=11111111%22222222%33333333 2021-12-01T03:54:56.672346Z,165,200,200,200,GET,https://api.test.io:443/service4/v1/jp/pay/accesstoken?device_id=AAAAA-BBBB-CCC ⬇️ URLの正規化（≒変換, 名寄せ）\nOutput:\ntimestamp,backend_processing_time_msec,alb_status_code,backend_status_code,method,URL 2021-12-01T03:50:00.115676Z,182.0,201,201,POST,https://api.test.io:443/service1/v2/jp/cart/{cart_no}/details 2021-12-01T03:50:20.597508Z,67.0,200,200,GET,https://api.test.io:443/service2/v2/jp/history/{order_no} 2021-12-01T03:50:20.613452Z,145.0,200,200,GET,https://api.test.io:443/service2/v2/jp/history 2021-12-01T03:50:20.894114Z,22.0,200,200,GET,https://api.test.io:443/service2/v2/jp/history/{order_no} 2021-12-01T03:51:45.903017Z,8.0,404,404,DELETE,https://api.test.io:443/service3/v1/jp/reserve/{cart_no} 2021-12-01T03:54:41.598315Z,20.0,200,200,GET,https://api.test.io:443/service3/v1/jp/stocks 2021-12-01T03:54:56.672346Z,165.0,200,200,GET,https://api.test.io:443/service4/v1/jp/pay/accesstoken 上記のような変換を行うことで、APIのEndpointごとにコール回数やレスポンス時間など統計処理をPandasなどを行うことができる。\nコード 概要で説明したような事を実現するために、以下のコードを作成。\n1. URLの変換パターンを定義した設定ファイル（JSON） 置換のパターンはAPIの定義書などを参考に記述するのが良い（ない場合はアクセスログからリバースするしかない\u0026hellip;） { \u0026#34;^(.*)/history/[0-9]{12,19}-[0-9]{5,7}\u0026#34;: \u0026#34;\\\\1/history/{order_no}\u0026#34;, \u0026#34;^(.*)(cart|reserve)/[0-9a-z]{32}\u0026#34;: \u0026#34;\\\\1\\\\2/{cart_no}\u0026#34; } 2. inputしたCSVからURLの変換処理を行うスクリプト（Python） タイトル詐欺になるが、クエリパラメータに関する置換はルールが単純なのでJSONでは定義せず、こちらのスクリプトで定義・処理している（# REMOVE QUERY PARAMETERのセクション）。 import pandas as pd import json input_filename = \u0026#39;./input.csv\u0026#39; output_filename = \u0026#39;./output.csv\u0026#39; input_csv = pd.read_csv( input_filename, sep=\u0026#39;,\u0026#39;, usecols=lambda x: x in [\u0026#39;timestamp\u0026#39;,\u0026#39;alb_status_code\u0026#39;, \u0026#39;backend_status_code\u0026#39;, \u0026#39;method\u0026#39;, \u0026#39;URL\u0026#39;, \u0026#39;backend_processing_time_msec\u0026#39;], index_col=\u0026#39;timestamp\u0026#39;, dtype={\u0026#39;timestamp\u0026#39;: str, \u0026#39;alb_status_code\u0026#39;: str, \u0026#39;backend_status_code\u0026#39;: str, \u0026#39;method\u0026#39;: str, \u0026#39;URL\u0026#39;: str, \u0026#39;backend_processing_time_msec\u0026#39;: \u0026#39;float16\u0026#39;} ) # REMOVE QUERY PARAMETER input_csv = pd.","title":"JSONで定義したURLの置換ルールをPythonで変換する"},{"content":" install 済みのパッケージ pip list\n特定パッケージのインストール場所などの詳細情報 pip show jupyter\n","permalink":"https://ny1030.github.io/pages/posts/engineering/pip-commands/","summary":"install 済みのパッケージ pip list\n特定パッケージのインストール場所などの詳細情報 pip show jupyter","title":"pip Commands"},{"content":"imageDigest を確認 e.g. apple というECRリポジトリの developタグ aws ecr describe-images --repository-name apple --image-ids imageTag=develop | jq .imageDetails[].imageDigest イメージのPull 1. 認証 aws ecr get-login-password --region ap-northeast-1 | docker login --username AWS --password-stdin 12345678.dkr.ecr.ap-northeast-1.amazonaws.com 補足 AWSアカウントIDは例として12345678とする ECRの接続情報はFQDNまたはURL（htttps://~）どちらの表記でも上手くいった。 --region で指定するリージョンは、ECRリポジトリと同じリージョンにする（us-east-1では無いので注意） ecr:GetAuthorizationToken のRoleが必要なので事前に権限付与しておく TroubleShooting 以下のエラーが出る場合がある\nError saving credentials: error storing credentials - err: exit status 1, out: `Post \u0026#34;http://ipc/registry/credstore-updated\u0026#34;: dial unix Library/Containers/com.docker.docker/Data/backend.sock: connect: connection refused` Qiitaの記事 を参考に、$HOME/.docker/config.json の credsStore の項目を削除することでエラーが消えることを確認（ただし、あくまで応急的な対応）\n2. Pull docker pull 12345678.dkr.ecr.ap-northeast-1.amazonaws.com/{repository_name}/{image_name}:{tag_name} ↓ docker images でPullできたことを確認 補足 ecr:BatchGetImage のRoleが必要 ","permalink":"https://ny1030.github.io/pages/posts/engineering/aws-ecr-commands/","summary":"imageDigest を確認 e.g. apple というECRリポジトリの developタグ aws ecr describe-images --repository-name apple --image-ids imageTag=develop | jq .imageDetails[].imageDigest イメージのPull 1. 認証 aws ecr get-login-password --region ap-northeast-1 | docker login --username AWS --password-stdin 12345678.dkr.ecr.ap-northeast-1.amazonaws.com 補足 AWSアカウントIDは例として12345678とする ECRの接続情報はFQDNまたはURL（htttps://~）どちらの表記でも上手くいった。 --region で指定するリージョンは、ECRリポジトリと同じリージョンにする（us-east-1では無いので注意） ecr:GetAuthorizationToken のRoleが必要なので事前に権限付与しておく TroubleShooting 以下のエラーが出る場合がある\nError saving credentials: error storing credentials - err: exit status 1, out: `Post \u0026#34;http://ipc/registry/credstore-updated\u0026#34;: dial unix Library/Containers/com.docker.docker/Data/backend.sock: connect: connection refused` Qiitaの記事 を参考に、$HOME/.docker/config.json の credsStore の項目を削除することでエラーが消えることを確認（ただし、あくまで応急的な対応）\n2. Pull docker pull 12345678.dkr.ecr.ap-northeast-1.amazonaws.com/{repository_name}/{image_name}:{tag_name} ↓ docker images でPullできたことを確認 補足 ecr:BatchGetImage のRoleが必要 ","title":"AWS-CLI ECR Commands"},{"content":"やりたいこと 運用しているシステムで時折、CPU使用率が100%を超過しアクセス不可になる。 Compute Engine（GCE）のインスタンスを再起動することで上記事象は直るので、復旧時間を短くするために、通知することでできるだけ早く気付けるようにする。 通知先として、スマホから手軽に見れてWebhookで楽に設定できるDiscordを使ってみる。 通知のロジックを超カンタンな図で表すと以下のような形： やったこと 通知ポリシーの作成（GCP管理画面） GCEのCPU使用率が95%以上の状態が何分間以上続いたら通知するか、といった条件をGCPの管理画面から設定する。これはGCEの画面のオブザーバビリティのタブから画像のように設定。 通知チャンネルの作成（GCP管理画面） 通知ポリシーと似た名前だがこちらは通知先を設定するサービス。画像の通りSlackやWebhook、（見えてないけど）SMSやEmailの設定が可能。今回はDiscordに通知したかったので通知したいDiscordチャンネルのWebhook URLを設定。 トラブルシューティング 上記の設定を終えたところで、通知チャンネルから「TEST CONNECTION」があったので試しに実行。 「successfully sent」と出ているがDiscordチャンネルを見るとメッセージが届いてない。。\nGCP側のログ確認 Cloud Logging で 確認したところ、同タイミングで400エラーが出ていることを確認 😇 curlで送ってみる 件のDiscordにcurlで試しにPOSTをしてみたところ、\n$ curl -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;data\u0026#34;: \u0026#34;Hello World\u0026#34;}\u0026#39; https://discord.com/api/webhooks/{YOUR_PATH} {\u0026#34;message\u0026#34;: \u0026#34;Cannot send an empty message\u0026#34;, \u0026#34;code\u0026#34;: 50006} というメッセージが返ってきた。status codeを調べると400なのでGCPと同じエラーっぽい。\n微修正して以下のようなPOSTをしたところ通知が成功。\n$ curl -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;content\u0026#34;: \u0026#34;Hello World\u0026#34;}\u0026#39; https://discord.com/api/webhooks/{YOUR_PATH} どうやらdiscordではJSONのpayloadに設定する key が \u0026ldquo;content\u0026rdquo; じゃないとエラーになる模様。なのでGCPではおそらく、content以外の key名を設定していてエラーが返ってきている？\n最終的にCloud Function で実装 以上のような経緯でWebhookで単純に送ることはできなかったため、同じような人がいないか調べたところ Cloud Functions で自前で作るのが良いとのこと。以下のような手順でCloud Functionsで実装を試してみた。\nCloud Pub/Subのトピックを作成 通知チャンネルで通知先に Cloud Pub/Sub -\u0026gt; 上記トピックを選択 Cloud Functionsを作成 トリガーのタイプ：Cloud Pub/Sub ランタイム：Nodejs ソースは以下の通り const { IncomingWebhook } = require(\u0026#39;@slack/webhook\u0026#39;); /** * Triggered from a message on a Cloud Pub/Sub topic. * * @param {!Object} event Event payload. * @param {!Object} context Metadata for the event. */ exports.notify = (event, context) =\u0026gt; { const message = event.data ? Buffer.from(event.data, \u0026#39;base64\u0026#39;).toString() : \u0026#39;Hello, World\u0026#39;; console.log(message); const body = { \u0026#34;content\u0026#34;: message }; // discordに通知 const webhook = new IncomingWebhook(\u0026#39;https://discord.com/api/webhooks/{YOUR_PATH}\u0026#39;); //await webhook.send(body); webhook.send(body); return \u0026#39;Discord notification sent successfully\u0026#39;; }; package.json\n{ \u0026#34;name\u0026#34;: \u0026#34;sample-pubsub\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.0.1\u0026#34;, \u0026#34;dependencies\u0026#34;: { \u0026#34;@google-cloud/pubsub\u0026#34;: \u0026#34;^0.18.0\u0026#34;, \u0026#34;@slack/webhook\u0026#34;: \u0026#34;^6.1.0\u0026#34; } } このFunctionをDeployして、再度テストしたところ無事に送信されたことを確認 ","permalink":"https://ny1030.github.io/pages/posts/engineering/gcp%E3%81%AE%E3%82%A2%E3%83%A9%E3%83%BC%E3%83%88%E6%83%85%E5%A0%B1%E3%82%92discord%E3%81%AB%E9%80%9A%E7%9F%A5%E3%81%99%E3%82%8B/","summary":"やりたいこと 運用しているシステムで時折、CPU使用率が100%を超過しアクセス不可になる。 Compute Engine（GCE）のインスタンスを再起動することで上記事象は直るので、復旧時間を短くするために、通知することでできるだけ早く気付けるようにする。 通知先として、スマホから手軽に見れてWebhookで楽に設定できるDiscordを使ってみる。 通知のロジックを超カンタンな図で表すと以下のような形： やったこと 通知ポリシーの作成（GCP管理画面） GCEのCPU使用率が95%以上の状態が何分間以上続いたら通知するか、といった条件をGCPの管理画面から設定する。これはGCEの画面のオブザーバビリティのタブから画像のように設定。 通知チャンネルの作成（GCP管理画面） 通知ポリシーと似た名前だがこちらは通知先を設定するサービス。画像の通りSlackやWebhook、（見えてないけど）SMSやEmailの設定が可能。今回はDiscordに通知したかったので通知したいDiscordチャンネルのWebhook URLを設定。 トラブルシューティング 上記の設定を終えたところで、通知チャンネルから「TEST CONNECTION」があったので試しに実行。 「successfully sent」と出ているがDiscordチャンネルを見るとメッセージが届いてない。。\nGCP側のログ確認 Cloud Logging で 確認したところ、同タイミングで400エラーが出ていることを確認 😇 curlで送ってみる 件のDiscordにcurlで試しにPOSTをしてみたところ、\n$ curl -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;data\u0026#34;: \u0026#34;Hello World\u0026#34;}\u0026#39; https://discord.com/api/webhooks/{YOUR_PATH} {\u0026#34;message\u0026#34;: \u0026#34;Cannot send an empty message\u0026#34;, \u0026#34;code\u0026#34;: 50006} というメッセージが返ってきた。status codeを調べると400なのでGCPと同じエラーっぽい。\n微修正して以下のようなPOSTをしたところ通知が成功。\n$ curl -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;content\u0026#34;: \u0026#34;Hello World\u0026#34;}\u0026#39; https://discord.com/api/webhooks/{YOUR_PATH} どうやらdiscordではJSONのpayloadに設定する key が \u0026ldquo;content\u0026rdquo; じゃないとエラーになる模様。なのでGCPではおそらく、content以外の key名を設定していてエラーが返ってきている？\n最終的にCloud Function で実装 以上のような経緯でWebhookで単純に送ることはできなかったため、同じような人がいないか調べたところ Cloud Functions で自前で作るのが良いとのこと。以下のような手順でCloud Functionsで実装を試してみた。","title":"GCPのアラート情報をDiscordに通知する"},{"content":"こちら\n","permalink":"https://ny1030.github.io/pages/posts/engineering/webhook%E3%81%AE%E7%96%8E%E9%80%9A%E3%83%86%E3%82%B9%E3%83%88/","summary":"こちら","title":"Webhookの疎通テスト"},{"content":"from __future__ import print_function import pickle import os.path from googleapiclient.discovery import build from google_auth_oauthlib.flow import InstalledAppFlow from google.auth.transport.requests import Request # If modifying these scopes, delete the file token.pickle. SCOPES = [\u0026#39;https://www.googleapis.com/auth/drive.metadata.readonly\u0026#39;] def main(): \u0026#34;\u0026#34;\u0026#34;Shows basic usage of the Drive v3 API. Prints the names and ids of the first 10 files the user has access to. \u0026#34;\u0026#34;\u0026#34; creds = None # The file token.pickle stores the user\u0026#39;s access and refresh tokens, and is # created automatically when the authorization flow completes for the first # time. if os.path.exists(\u0026#39;token.pickle\u0026#39;): with open(\u0026#39;token.pickle\u0026#39;, \u0026#39;rb\u0026#39;) as token: creds = pickle.load(token) # If there are no (valid) credentials available, let the user log in. if not creds or not creds.valid: if creds and creds.expired and creds.refresh_token: creds.refresh(Request()) else: flow = InstalledAppFlow.from_client_secrets_file( \u0026#39;credentials.json\u0026#39;, SCOPES) creds = flow.run_local_server(port=0) # Save the credentials for the next run with open(\u0026#39;token.pickle\u0026#39;, \u0026#39;wb\u0026#39;) as token: pickle.dump(creds, token) service = build(\u0026#39;drive\u0026#39;, \u0026#39;v3\u0026#39;, credentials=creds) # Call the Drive v3 API results = service.files().list( pageSize=10, fields=\u0026#34;nextPageToken, files(id, name)\u0026#34;).execute() items = results.get(\u0026#39;files\u0026#39;, []) if not items: print(\u0026#39;No files found.\u0026#39;) else: print(\u0026#39;Files:\u0026#39;) for item in items: print(u\u0026#39;{0} ({1})\u0026#39;.format(item[\u0026#39;name\u0026#39;], item[\u0026#39;id\u0026#39;])) if __name__ == \u0026#39;__main__\u0026#39;: main() 参考サイト\n","permalink":"https://ny1030.github.io/pages/posts/engineering/google-drive%E3%81%AB%E3%81%82%E3%82%8B%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%82%92ocr%E3%81%99%E3%82%8Bpython%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%97%E3%83%88/","summary":"from __future__ import print_function import pickle import os.path from googleapiclient.discovery import build from google_auth_oauthlib.flow import InstalledAppFlow from google.auth.transport.requests import Request # If modifying these scopes, delete the file token.pickle. SCOPES = [\u0026#39;https://www.googleapis.com/auth/drive.metadata.readonly\u0026#39;] def main(): \u0026#34;\u0026#34;\u0026#34;Shows basic usage of the Drive v3 API. Prints the names and ids of the first 10 files the user has access to. \u0026#34;\u0026#34;\u0026#34; creds = None # The file token.pickle stores the user\u0026#39;s access and refresh tokens, and is # created automatically when the authorization flow completes for the first # time.","title":"Google DriveにあるファイルをOCRするPythonスクリプト"},{"content":"freeコマンド total : 合計 used : カーネルとプロセスが使用している shared : tmpfsに使われている free : 余っている buffers : バッファキャッシュのメモリサイズ cache : ページキャッシュのメモリサイズ available : 実質的な空きメモリ free + buff/cache (解放可能な部分) sar コマンド 物理メモリ使用量が表示される = buffers, cacheが含まれた使用量 sar -r でメモリ使用状況を確認する - ablog\n","permalink":"https://ny1030.github.io/pages/posts/engineering/linux-%E3%83%A1%E3%83%A2%E3%83%AA%E4%BD%BF%E7%94%A8%E7%8E%87%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6/","summary":"freeコマンド total : 合計 used : カーネルとプロセスが使用している shared : tmpfsに使われている free : 余っている buffers : バッファキャッシュのメモリサイズ cache : ページキャッシュのメモリサイズ available : 実質的な空きメモリ free + buff/cache (解放可能な部分) sar コマンド 物理メモリ使用量が表示される = buffers, cacheが含まれた使用量 sar -r でメモリ使用状況を確認する - ablog","title":"Linux - メモリ使用率について"},{"content":"gist\n","permalink":"https://ny1030.github.io/pages/posts/engineering/commandsnipet%E3%83%A1%E3%83%A2/","summary":"gist","title":"Command(Snipet)メモ"},{"content":"Level 0 実行計画(Query Plan)の読み方 以下のようなSQLの実行計画を取得してみる。\nUPDATE T_ITEM_INBOUND t3 SET status_flag = \u0026#39;9\u0026#39; , updated_datetime = CURRENT_TIMESTAMP , updated_by = \u0026#39;APP_001\u0026#39; FROM ( SELECT t1.group_num , t1.level3_item_code FROM T_ITEM_INBOUND t1 WHERE t1.status_flag = \u0026#39;1\u0026#39; AND t1.group_num = \u0026#39;10\u0026#39; AND t1.level3_item_code = \u0026#39;1000-2000-3000\u0026#39; GROUP BY t1.group_num , t1.level1_item_code , t1.level2_item_code , t1.level3_item_code , t1.color_code , t1.size_code , t1.pattern_length_code HAVING 1 \u0026lt; COUNT(t1.group_num) ) t2 WHERE t3.status_flag = \u0026#39;1\u0026#39; AND t3.group_num = t2.group_num AND t3.level3_item_code = t2.level3_item_code AND t3.brand_code = \u0026#39;TK\u0026#39; AND t3.region_code = \u0026#39;JP\u0026#39; 上記クエリの先頭に EXPLAIN ANALYZE というフレーズを付与→実行することで、以下のように実行計画が出力される。\nUpdate on T_ITEM_INBOUND t3 (cost=19152.43..40432.82 rows=1 width=299) (actual time=65.760..65.760 rows=0 loops=1) -\u0026gt; Hash Join (cost=19152.43..40432.82 rows=1 width=299) (actual time=65.758..65.758 rows=0 loops=1) Hash Cond: (((t3.group_num)::text = (t2.group_num)::text) AND (t3.level3_item_code = t2.level3_item_code)) -\u0026gt; Seq Scan on T_ITEM_INBOUND t3 (cost=0.00..19152.36 rows=283735 width=150) (actual time=4.019..4.019 rows=1 loops=1) Filter: (((status_flag)::text = \u0026#39;1\u0026#39;::text) AND (brand_code = \u0026#39;TK\u0026#39;::text) AND (region_code = \u0026#39;JP\u0026#39;::text)) Rows Removed by Filter: 11 -\u0026gt; Hash (cost=19152.42..19152.42 rows=1 width=62) (actual time=61.731..61.731 rows=0 loops=1) Buckets: 1024 Batches: 1 Memory Usage: 8kB -\u0026gt; Subquery Scan on t2 (cost=19152.37..19152.42 rows=1 width=62) (actual time=61.730..61.730 rows=0 loops=1) -\u0026gt; GroupAggregate (cost=19152.37..19152.41 rows=1 width=55) (actual time=61.730..61.730 rows=0 loops=1) Group Key: t1.group_num, t1.level1_item_code, t1.level2_item_code, t1.level3_item_code, t1.color_code, t1.size_code, t1.pattern_length_code Filter: (1 \u0026lt; count(t1.group_num)) Rows Removed by Filter: 1 -\u0026gt; Sort (cost=19152.37..19152.38 rows=1 width=55) (actual time=61.724..61.724 rows=1 loops=1) Sort Key: t1.level1_item_code, t1.level2_item_code, t1.color_code, t1.size_code, t1.pattern_length_code Sort Method: quicksort Memory: 25kB -\u0026gt; Seq Scan on T_ITEM_INBOUND t1 (cost=0.00..19152.36 rows=1 width=55) (actual time=21.804..61.714 rows=1 loops=1) Filter: (((status_flag)::text = \u0026#39;1\u0026#39;::text) AND ((group_num)::text = \u0026#39;10\u0026#39;::text) AND (l3_item_code = \u0026#39;1000-2000-3000\u0026#39;::text)) Rows Removed by Filter: 283734 全てを説明するのは長くなるので要点だけ。\nインデントは処理の順番を表す 処理を行う単位（=実行計画の各レコード）ノードと呼ぶ。 ノードはツリー構造で表現される。最下層のノード（=リーフノード）は必ずテーブルスキャンノードとなる。 最下層（=インデントが深い）のノードから順に実行される。 最下層ノードの親は結合ノードでさらにその親はその他のノードになっている。 テーブルスキャンノード テーブルからデータを取り出す役割。代表的なスキャン方法は以下の通り。\nSeq Scan: テーブル全体を順番にスキャンする Index Scan: テーブルに付与されているインデックスのみをスキャンし、実テーブルはスキャンしない 今回の例では Seq Scan が使われいている。\n結合系ノード 複数のテーブルを結合する役割のノードです。代表的な結合方法は以下の通り。\nNested Loop: 外側テーブルの行毎に内側テーブルのすべての行を突き合わせ結合する Hash Join: 内側テーブルの結合キーでハッシュを作成し、ハッシュと外側テーブルの結合キーで一致する行を結合する 処理コスト 実行計画の各ノードには、始動コストと総コスト、行数と行の長さが記載されている。\n始動コスト: 一件目のデータを返すのにかかる想定のコストを表す( .. の前) 総コスト: 処理完了までにかかる想定のコストを表す( .. の後) 行数： ノード実行によって返却される行数を表す( rows ) 行の長さ： ノードの実行によって返却される行の平均の長さを表す( width ) important EXPLAIN のみをつけて実行計画を取得すると、プランナによって見積もられたコストとなる。 EXPLAIN ANALYZE をつけて実行計画を取得すると、実際に実行して得られた結果となる。( actual 以降の情報がそれ) なので、 cost と actual の rows が大きくずれている場合は統計情報が古い可能性がある。\nLevel 1 Index利用による高速化 Level 2 部分Indexの利用 ","permalink":"https://ny1030.github.io/pages/posts/engineering/sql%E3%83%91%E3%83%95%E3%82%A9%E3%83%BC%E3%83%9E%E3%83%B3%E3%82%B9%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0/","summary":"Level 0 実行計画(Query Plan)の読み方 以下のようなSQLの実行計画を取得してみる。\nUPDATE T_ITEM_INBOUND t3 SET status_flag = \u0026#39;9\u0026#39; , updated_datetime = CURRENT_TIMESTAMP , updated_by = \u0026#39;APP_001\u0026#39; FROM ( SELECT t1.group_num , t1.level3_item_code FROM T_ITEM_INBOUND t1 WHERE t1.status_flag = \u0026#39;1\u0026#39; AND t1.group_num = \u0026#39;10\u0026#39; AND t1.level3_item_code = \u0026#39;1000-2000-3000\u0026#39; GROUP BY t1.group_num , t1.level1_item_code , t1.level2_item_code , t1.level3_item_code , t1.color_code , t1.size_code , t1.pattern_length_code HAVING 1 \u0026lt; COUNT(t1.group_num) ) t2 WHERE t3.status_flag = \u0026#39;1\u0026#39; AND t3.group_num = t2.group_num AND t3.","title":"SQLパフォーマンスチューニング"},{"content":"サービス再起動 bitnami drupal の場合 sudo /opt/bitnami/ctlscript.sh restart マイナーバージョンアップ ※スペック低すぎると失敗するので、4コア8GBに事前スケールアップした\nバージョン確認 composer outdated \u0026quot;drupal/*\u0026quot;\ndruplaのrootディレクトリで以下を実行 sudo composer update drupal/core \u0026quot;drupal/core-*\u0026quot; --with-all-dependencies sudo composer update drupal/core --with-dependencies\n参考\nhousekeeping(キャッシュ削除) drush -y wd-del all\nリダイレクト設定 こちらの通りに設定 installdir/apache2/conf/vhosts/APPNAME-vhost.conf に設定を書けば良い\n","permalink":"https://ny1030.github.io/pages/posts/engineering/drupal%E9%96%A2%E9%80%A3%E3%81%AE%E3%82%B3%E3%83%9E%E3%83%B3%E3%83%89/","summary":"サービス再起動 bitnami drupal の場合 sudo /opt/bitnami/ctlscript.sh restart マイナーバージョンアップ ※スペック低すぎると失敗するので、4コア8GBに事前スケールアップした\nバージョン確認 composer outdated \u0026quot;drupal/*\u0026quot;\ndruplaのrootディレクトリで以下を実行 sudo composer update drupal/core \u0026quot;drupal/core-*\u0026quot; --with-all-dependencies sudo composer update drupal/core --with-dependencies\n参考\nhousekeeping(キャッシュ削除) drush -y wd-del all\nリダイレクト設定 こちらの通りに設定 installdir/apache2/conf/vhosts/APPNAME-vhost.conf に設定を書けば良い","title":"Drupal関連のコマンド"},{"content":"※ OSは Ubuntu 20.04(LTS) を使用\nまずはプロンプトを変更（ミニマルな内容にする） #1.まずは現在の設定確認 user@DESKTOP-XXXXX:~$ **echo $PS1** \\\\[\\\\e]0;\\\\u@\\\\h: \\\\w\\\\a\\\\]${debian_chroot:+($debian_chroot)}\\\\[\\\\033[01;32m\\\\]\\\\u@\\\\h\\\\[\\\\033[00m\\\\]:\\\\[\\\\033[01;34m\\\\]\\\\w\\\\[\\\\033[00m\\\\]\\\\$ #2.PS1の値を書き換え user@DESKTOP-XXXXX:~$ **PS1=\u0026#39;\\\\[\\\\e[1;32m\\\\]\\\\W\\\\$ \\\\[\\\\e[m\\\\]\u0026#39;** ~$ #3.設定内容を永続化 ~$ **echo \u0026#34;PS1=\u0026#39;\\\\[\\\\e[1;32m\\\\]\\\\W\\\\$ \\\\[\\\\e[m\\\\]\u0026#39;\u0026#34; \u0026gt;\u0026gt; .bashrc** #4.ターミナルを再起動して、設定が永続化されてること確認 ~$ systemd を PID1（親プロセス）にする #1.現状のプロセスを確認 ~$ ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.0 896 528 ? Sl 16:13 0:00 /init root 120 0.0 0.0 896 84 ? Ss 16:19 0:00 /init #2.dotnet-runtime-5.0などの依存モジュールをインストール wget \u0026lt;https://packages.microsoft.com/config/ubuntu/20.04/packages-microsoft-prod.deb\u0026gt; -O packages-microsoft-prod.deb sudo dpkg -i packages-microsoft-prod.deb rm packages-microsoft-prod.deb sudo apt-get update; \\\\ sudo apt-get install -y apt-transport-https \u0026amp;\u0026amp; \\\\ sudo apt-get update \u0026amp;\u0026amp; \\\\ sudo apt-get install -y aspnetcore-runtime-5.0 sudo apt install -y daemonize dbus gawk libc6 libstdc++6 policykit-1 systemd systemd-container sudo -s apt install apt-transport-https wget -O /etc/apt/trusted.gpg.d/wsl-transdebian.gpg \u0026lt;https://arkane-systems.github.io/wsl-transdebian/apt/wsl-transdebian.gpg\u0026gt; chmod a+r /etc/apt/trusted.gpg.d/wsl-transdebian.gpg cat \u0026lt;\u0026lt; EOF \u0026gt; /etc/apt/sources.list.d/wsl-transdebian.list deb \u0026lt;https://arkane-systems.github.io/wsl-transdebian/apt/\u0026gt; $(lsb_release -cs) main deb-src \u0026lt;https://arkane-systems.github.io/wsl-transdebian/apt/\u0026gt; $(lsb_release -cs) main EOF apt update #3.genieインストール sudo apt install -y systemd-genie #4.genieを実行 genie -s #5.エラー解消 #5-1.systemd-remount-fs.serviceのエラーを解消する dfでルートパーティション確認→/dev/sdb sudo e2label /dev/sdb cloudimg-rootfs #5-2.multipathd.socketのエラーを解消する sudo systemctl disable multipathd.socket #5-3.ssh.serviceのエラーを解消する sudo ssh-keygen -A #6.bashrcに設定記入 # Are we in the bottle? if [[ ! -v INSIDE_GENIE ]]; then echo \u0026#34;Starting genie:\u0026#34; exec /usr/bin/genie -s fi #7.Ubuntu再起動後にPID1を確認 ~$ ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.2 0.1 175028 13128 ? Ss 21:40 0:01 systemd root 50 0.0 0.1 51456 15596 ? S\u0026lt;s 21:40 0:00 /lib/systemd/systemd-journald root 72 0.0 0.0 19564 5228 ? Ss 21:40 0:00 /lib/systemd/systemd-udevd 補足：systemdをPID1にしないと、systemctlを実行すると以下のようなエラーが出る\nSystem has not been booted with systemd as init system (PID 1). Can\u0026#39;t operate. Failed to connect to bus: Host is down WindowsのPATHを引き継がないように設定 1. 設定の追加 sudo vi /etc/wsl.conf -- # WindowsのPATHを引き継がない設定を追記する [interop] appendWindowsPath = false 2. Widnowsコマンドプロンプトから再起動 -- wsl.exe --shutdown Golangをインストール # 1. Goのパッケージをダウンロード ~$ wget \u0026lt;https://go.dev/dl/go1.17.7.linux-amd64.tar.gz\u0026gt; # 2. 展開 ~$ sudo tar -C /usr/local -xzf go1.17.7.linux-amd64.tar.gz # 3. PATHに追加・反映 ~$ echo \u0026#34;PATH=$PATH:/usr/local/go/bin\u0026#34; \u0026gt;\u0026gt; .bashrc ~$ source .bashrc # 4．動作確認 ~$ go version go version go1.17.7 linux/amd64 ghq（Gitリポジトリ管理ツール）をインストール go install github.com/x-motemen/ghq@latest echo \u0026#34;PATH=$PATH:~/go/bin\u0026#34; \u0026gt;\u0026gt; .bashrc # dial tcp: lookup proxy.golang.org: no such host のエラー出た場合 go env -w GOPROXY=direct AWS CLI 2をインストール curl \u0026#34;\u0026lt;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026gt;\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install ~$ aws --version aws-cli/2.4.23 Python/3.8.8 Linux/5.10.60.1-microsoft-standard-WSL2 exe/x86_64.ubuntu.20 prompt/off AWS Session Managerをインストール curl \u0026#34;\u0026lt;https://s3.amazonaws.com/session-manager-downloads/plugin/latest/ubuntu_64bit/session-manager-plugin.deb\u0026gt;\u0026#34; -o \u0026#34;session-manager-plugin.deb\u0026#34; sudo dpkg -i session-manager-plugin.deb ~$ session-manager-plugin The Session Manager plugin was installed successfully. Use the AWS CLI to start a session. Proxy環境下の関連設定 zscalerの場合 Linuxで証明書導入 /usr/share/ca-certificates/ 配下に任意のディレクトリ作成 /usr/share/ca-certificates/zscaler/ 配下に証明書を配置 /etc/ca-certificates.confに/usr/share/ca-certificates/ 以下の相対パスを追加 （ここではzscaler/zscaler.cer） /usr/sbin/update-ca-certificatesを実行 aws export AWS_CA_BUNDLE=~/zscaler_root.crt git, conda, pip １．最新のPythonの信頼する証明書リストを入手する。\u0026lt;https://curl.haxx.se/ca/cacert.pemからダウンロードする。\u0026gt; ２．このファイルの末尾に、テキスト形式で保存したzscalerのオレオレ証明書のテキストを追加する。 ３．以下のパスに保存する# Windows%USERPROFILE%\\\\certs\\\\ca-bundle.crt ４．以下のコマンドをコマンドプロンプトで実行し、保存した証明書リストを使うように設定する。 pip config set global.cert %USERPROFILE%\\\\certs\\\\ca-bundle.crtconda config --set ssl_verify %USERPROFILE%\\\\certs\\\\ca-bundle.crtgit config --global http.sslVerify truegit config --global http.sslCAInfo path/to/ca-bundle.crt DNS Resolver に8.8.8.8追加 #1. /etc/systemd/resolved.conf に以下を追加 --------------- [Resolve] DNS=8.8.8.8 #2.再起動 systemctl restart systemd-resolved #3.DNS確認 systemd-resolve --status ","permalink":"https://ny1030.github.io/pages/posts/engineering/wsl2%E3%82%92%E5%85%A5%E3%82%8C%E3%81%9F%E5%BE%8C%E3%81%AB%E3%82%84%E3%82%8B%E3%82%AB%E3%82%B9%E3%82%BF%E3%83%9E%E3%82%A4%E3%82%BA/","summary":"※ OSは Ubuntu 20.04(LTS) を使用\nまずはプロンプトを変更（ミニマルな内容にする） #1.まずは現在の設定確認 user@DESKTOP-XXXXX:~$ **echo $PS1** \\\\[\\\\e]0;\\\\u@\\\\h: \\\\w\\\\a\\\\]${debian_chroot:+($debian_chroot)}\\\\[\\\\033[01;32m\\\\]\\\\u@\\\\h\\\\[\\\\033[00m\\\\]:\\\\[\\\\033[01;34m\\\\]\\\\w\\\\[\\\\033[00m\\\\]\\\\$ #2.PS1の値を書き換え user@DESKTOP-XXXXX:~$ **PS1=\u0026#39;\\\\[\\\\e[1;32m\\\\]\\\\W\\\\$ \\\\[\\\\e[m\\\\]\u0026#39;** ~$ #3.設定内容を永続化 ~$ **echo \u0026#34;PS1=\u0026#39;\\\\[\\\\e[1;32m\\\\]\\\\W\\\\$ \\\\[\\\\e[m\\\\]\u0026#39;\u0026#34; \u0026gt;\u0026gt; .bashrc** #4.ターミナルを再起動して、設定が永続化されてること確認 ~$ systemd を PID1（親プロセス）にする #1.現状のプロセスを確認 ~$ ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.0 896 528 ? Sl 16:13 0:00 /init root 120 0.0 0.0 896 84 ? Ss 16:19 0:00 /init #2.dotnet-runtime-5.0などの依存モジュールをインストール wget \u0026lt;https://packages.microsoft.com/config/ubuntu/20.04/packages-microsoft-prod.deb\u0026gt; -O packages-microsoft-prod.","title":"WSL2を入れた後にやるカスタマイズ"},{"content":"１．環境セットアップ（２時間くらい） Flutter １．SDKのダウンロード・解凍\nInstall\n２．PATH設定\n以下をPATHに追加（ホームディレクトリの名前は適宜変更する）\n/Users/{username}/flutter/bin ３．確認\n以下のコマンドが実行できることを確認する\nflutter Android １．Android Studioダウンロード⇛インストール\nInstall\n２．確認\nflutter doctor ※最初は５分くらい時間かかる\nの結果でAndroid Studioがチェックマーク入ってるのを確認 ４．Andoroid toolchainのエラー解消\n・Command-Line Toolsのインストール（AndoroidSDKの設定画面からできる）\n・Andoroidライセンスの許可\nflutter doctor --android-licenses ※ javaのPATH通ってないとエラー出る\n５．Android toolchainがチェックついてることを確認\nflutter doctor ６．FlutterプラグインをAndoroid Studioからインストール\nXCode（iOS） １．XCodeをインストール\n２．Cocoapodをインストール\nbrew install cocoapods brew link --overwrite cocoapods ※バージョン指定している理由はエラー回避しようとした結果\n３．XCodeにチェックがついてることを確認\nflutter doctor ２．Flutterプロジェクト作成・デモアプリ実行（30分） １．以下のように作成\n２．仮想デバイス（iOS）を実装\n３．仮想デバイス（Andoroid）を実装 System Image：何でもいいらしいのでPieに ４．アプリの実行 Android： iOS：同じ見た目 Appendix．UIのデザインツール ３−１．Flutter Studio\nWEB上でデザイン⇛コード生成できるが、\n・慣れが必要そう、機能的にFigmaに劣ってそう\n・コードを完全には使えない（クラス名がデフォルト、構文？が違う）\nAppBuilder 2 20180529-19:35\n３−２．Figma to Flutter\n・生成されるコードがイマイチ\nFigma to Flutter\n３−３．Adobe XD\nこれは使ってないが、一番マシな気がする。ただし有償\n","permalink":"https://ny1030.github.io/pages/posts/engineering/flutter-%E5%B0%8E%E5%85%A5%E3%81%BE%E3%81%A8%E3%82%81/","summary":"１．環境セットアップ（２時間くらい） Flutter １．SDKのダウンロード・解凍\nInstall\n２．PATH設定\n以下をPATHに追加（ホームディレクトリの名前は適宜変更する）\n/Users/{username}/flutter/bin ３．確認\n以下のコマンドが実行できることを確認する\nflutter Android １．Android Studioダウンロード⇛インストール\nInstall\n２．確認\nflutter doctor ※最初は５分くらい時間かかる\nの結果でAndroid Studioがチェックマーク入ってるのを確認 ４．Andoroid toolchainのエラー解消\n・Command-Line Toolsのインストール（AndoroidSDKの設定画面からできる）\n・Andoroidライセンスの許可\nflutter doctor --android-licenses ※ javaのPATH通ってないとエラー出る\n５．Android toolchainがチェックついてることを確認\nflutter doctor ６．FlutterプラグインをAndoroid Studioからインストール\nXCode（iOS） １．XCodeをインストール\n２．Cocoapodをインストール\nbrew install cocoapods brew link --overwrite cocoapods ※バージョン指定している理由はエラー回避しようとした結果\n３．XCodeにチェックがついてることを確認\nflutter doctor ２．Flutterプロジェクト作成・デモアプリ実行（30分） １．以下のように作成\n２．仮想デバイス（iOS）を実装\n３．仮想デバイス（Andoroid）を実装 System Image：何でもいいらしいのでPieに ４．アプリの実行 Android： iOS：同じ見た目 Appendix．UIのデザインツール ３−１．Flutter Studio\nWEB上でデザイン⇛コード生成できるが、\n・慣れが必要そう、機能的にFigmaに劣ってそう\n・コードを完全には使えない（クラス名がデフォルト、構文？が違う）\nAppBuilder 2 20180529-19:35\n３−２．Figma to Flutter","title":"Flutter 導入まとめ"},{"content":"Case1 www無しドメイン -\u0026gt; www有りドメインにリダイレクト 参考：Apache RewriteCond の基礎知識 | - WEB ARCH LABO {apache_dir}/conf/vhost\n\u0026lt;VirtualHost *:80\u0026gt; ... \u0026lt;Directory \u0026#34;/opt/blog\u0026#34;\u0026gt; ... \u0026lt;/Directory\u0026gt; RewriteEngine On RewriteCond %{HTTP_HOST} !^www\\. [NC] RewriteCond %{REQUEST_URI} !^/admin/ RewriteRule ^(.*)$ http://www.%{HTTP_HOST}%{REQUEST_URI} [R=301,L] ... \u0026lt;/VirtualHost\u0026gt; RewriteEngine On: このディレクティブを書かないとRewriteは動かない。言い換えるとここだけコメントアウトすればRewriteを無効にできる。 RewriteCond: この条件に合致（true）したらRewriteRuleにしたがってURLの書き換えをおこなう。 今回の例：サーバのホスト名%{HTTP_HOST} でwww.が先頭にない !^www\\.とき。（大文字小文字は区別しない[NC]） RewriteCondを複数記述した場合はデフォルトではAND条件になる。ORにしたい場合は末尾に [OR] を指定する。例では2つ目のRewriteCondにおいて /admin のパスはリライトしないように条件を追加。 RewriteRule: URLの書き換えルールを書いている。リダイレクトのステータスコードを末尾に記載。 [L] をつけると振り分けが終わる（それ以降の処理は無視される）が、Rewrite後に再度評価が実行されるのでループに注意とのこと。再度評価をしたくない場合、[END] をつければいいとのこと。.htaccess に RewriteRule 書くときは、[L]フラグをつけても 併せてhttpsにリダイレクトしたい場合はRewriteRuleで指定しているプロトコルをhttpsにする。 Case2 www有りドメイン -\u0026gt; www無しドメインにリダイレクト {apache_dir}/conf/vhost\n\u0026lt;VirtualHost *:80\u0026gt; ... \u0026lt;Directory \u0026#34;/opt/blog\u0026#34;\u0026gt; ... \u0026lt;/Directory\u0026gt; RewriteEngine On RewriteCond %{HTTP_HOST} ^www\\.(.*)$ [NC] RewriteCond %{REQUEST_URI} !^/admin/ RewriteRule ^(.*)$ http://%1$1 [R=301,L] ... \u0026lt;/VirtualHost\u0026gt; %1 は RewriteCond で指定している (.*) キャプチャされたパターン。http://www.example.com の場合は example.com が入る。 補足 上記は http に対するリダイレクト設定。https向けには、\u0026lt;VirtualHost *:443\u0026gt; ディレクティブが定義されているファイル）に同じ設定を加える。 Case3 http -\u0026gt; httpsへリダイレクト {apache_dir}/conf/vhost\n\u0026lt;VirtualHost *:80\u0026gt; ... \u0026lt;Directory \u0026#34;/opt/blog\u0026#34;\u0026gt; ... \u0026lt;/Directory\u0026gt; RewriteEngine On RewriteCond %{HTTP_HOST} ^(.*)$ [NC] RewriteCond %{REQUEST_URI} !^/admin/ RewriteRule ^(.*)$ https://%1$1 [R=301,L] ... \u0026lt;/VirtualHost\u0026gt; Case2を若干変えた程度 ","permalink":"https://ny1030.github.io/pages/posts/engineering/apache%E3%81%AEredirect%E8%A8%AD%E5%AE%9A/","summary":"Case1 www無しドメイン -\u0026gt; www有りドメインにリダイレクト 参考：Apache RewriteCond の基礎知識 | - WEB ARCH LABO {apache_dir}/conf/vhost\n\u0026lt;VirtualHost *:80\u0026gt; ... \u0026lt;Directory \u0026#34;/opt/blog\u0026#34;\u0026gt; ... \u0026lt;/Directory\u0026gt; RewriteEngine On RewriteCond %{HTTP_HOST} !^www\\. [NC] RewriteCond %{REQUEST_URI} !^/admin/ RewriteRule ^(.*)$ http://www.%{HTTP_HOST}%{REQUEST_URI} [R=301,L] ... \u0026lt;/VirtualHost\u0026gt; RewriteEngine On: このディレクティブを書かないとRewriteは動かない。言い換えるとここだけコメントアウトすればRewriteを無効にできる。 RewriteCond: この条件に合致（true）したらRewriteRuleにしたがってURLの書き換えをおこなう。 今回の例：サーバのホスト名%{HTTP_HOST} でwww.が先頭にない !^www\\.とき。（大文字小文字は区別しない[NC]） RewriteCondを複数記述した場合はデフォルトではAND条件になる。ORにしたい場合は末尾に [OR] を指定する。例では2つ目のRewriteCondにおいて /admin のパスはリライトしないように条件を追加。 RewriteRule: URLの書き換えルールを書いている。リダイレクトのステータスコードを末尾に記載。 [L] をつけると振り分けが終わる（それ以降の処理は無視される）が、Rewrite後に再度評価が実行されるのでループに注意とのこと。再度評価をしたくない場合、[END] をつければいいとのこと。.htaccess に RewriteRule 書くときは、[L]フラグをつけても 併せてhttpsにリダイレクトしたい場合はRewriteRuleで指定しているプロトコルをhttpsにする。 Case2 www有りドメイン -\u0026gt; www無しドメインにリダイレクト {apache_dir}/conf/vhost\n\u0026lt;VirtualHost *:80\u0026gt; ... \u0026lt;Directory \u0026#34;/opt/blog\u0026#34;\u0026gt; ... \u0026lt;/Directory\u0026gt; RewriteEngine On RewriteCond %{HTTP_HOST} ^www\\.","title":"ApacheのRedirect/Rewrite設定"}]